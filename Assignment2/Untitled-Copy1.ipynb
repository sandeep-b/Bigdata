{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code.\n",
    "    You might find numpy functions np.exp, np.sum, np.reshape,\n",
    "    np.max, and numpy broadcasting useful for this task. (numpy\n",
    "    broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "    You should also make sure that your code works for one\n",
    "    dimensional inputs (treat the vector as a row), you might find\n",
    "    it helpful for your later problems.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the \n",
    "    written assignment!\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "        x -= np.min(x)  # solving overflow problem\n",
    "        x = np.exp(x)\n",
    "        x /= np.sum(x)\n",
    "    else:\n",
    "        x -= np.min(x, axis=1, keepdims=True)  # solving overflow problem\n",
    "        x = np.exp(x)\n",
    "        x /= np.sum(x, axis=1, keepdims=True)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print(\"Running basic tests...\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print(test1)\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print(test2)\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print(test3)\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print(\"You should verify these results!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should verify these results!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        ### YOUR CODE HERE:\n",
    "        \n",
    "        random.setstate(rndstate)\n",
    "        tmp1 = np.copy(x) \n",
    "        tmp1[ix] = tmp1[ix] + h\n",
    "        f1, _ = f(tmp1)\n",
    "        \n",
    "        random.setstate(rndstate)\n",
    "        tmp2 = np.copy(x) \n",
    "        tmp2[ix] = tmp2[ix] - h\n",
    "        f2, _ = f(tmp2)\n",
    "        numgrad = (f1 - f2) / (2 * h)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed.\")\n",
    "            print(\"First gradient error found at index %s\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print(\"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print(\"Running sanity checks...\")\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n",
      "You should verify these results!\n",
      "\n",
      "Running your tests...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    x = 1. / (1. + np.exp(-x))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input f should be the sigmoid\n",
    "    function value of your original input x. \n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    f = f * (1 - f)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f\n",
    "\n",
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print(\"Running basic tests...\")\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print(f)\n",
    "    assert np.amax(f - np.array([[0.73105858, 0.88079708], \n",
    "        [0.26894142, 0.11920292]])) <= 1e-6\n",
    "    print(g)\n",
    "    assert np.amax(g - np.array([[0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])) <= 1e-6\n",
    "    print(\"You should verify these results!\\n\")\n",
    "\n",
    "def test_sigmoid(): \n",
    "    \"\"\"\n",
    "    Use this space to test your sigmoid implementation by running:\n",
    "        python q2_sigmoid.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your tests be graded.\n",
    "    \"\"\"\n",
    "    print(\"Running your tests...\")\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sigmoid_basic();\n",
    "    test_sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "test 1 result: 8.414836786079764e-10\n",
      "test 2 result: 0.0\n",
      "test 3 result: -2.524451035823933e-09\n",
      "\n",
      "Running your sanity checks...\n"
     ]
    }
   ],
   "source": [
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"wb\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    # Implement the stochastic gradient descent method in this        \n",
    "    # function.                                                       \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - f: the function to optimize, it should take a single        \n",
    "    #     argument and yield two outputs, a cost and the gradient  \n",
    "    #     with respect to the arguments                            \n",
    "    # - x0: the initial point to start SGD from                     \n",
    "    # - step: the step size for SGD                                 \n",
    "    # - iterations: total iterations to run SGD for                 \n",
    "    # - postprocessing: postprocessing function for the parameters  \n",
    "    #     if necessary. In the case of word2vec we will need to    \n",
    "    #     normalize the word vectors to have unit length.          \n",
    "    # - PRINT_EVERY: specifies every how many iterations to output  \n",
    "\n",
    "    # Output:                                                         \n",
    "    # - x: the parameter value after SGD finishes  \n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        cost, grad = f(x)\n",
    "        x -= step * grad\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if PRINT_EVERY is not None and iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print(\"iter %d: %f\" % (iter, expcost))\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print(\"Running sanity checks...\")\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=None)\n",
    "    print(\"test 1 result:\", t1)\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=None)\n",
    "    print(\"test 2 result:\", t2)\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=None)\n",
    "    print(\"test 3 result:\", t3)\n",
    "    assert abs(t3) <= 1e-6\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "def your_sanity_checks(): \n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q3_sgd.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print(\"Running your sanity checks...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check();\n",
    "    your_sanity_checks();\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n",
      "Running your sanity checks...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\" \n",
    "    Forward and backward propagation for a two-layer sigmoidal network \n",
    "    \n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2]) # (dim_x, dim_h, dim_y)\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H)) # (dim_x, dim_h)\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H)) # (1, dim_h)\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy)) # (dim_h, dim_y)\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy)) # (1, dim_y)\n",
    " \n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    \n",
    "    h = sigmoid(np.dot(data, W1) + b1)  \n",
    "    pred = sigmoid(np.dot(h, W2) + b2) \n",
    "    cost = (-1) * np.sum(labels * np.log(pred) + (1 - labels) * np.log(1 - pred))  # sigmoid 함수를 썼을 때 cost function\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    \n",
    "    dout = pred - labels \n",
    "    dh = np.dot(dout, W2.T) * sigmoid_grad(h)  \n",
    "    \n",
    "    gradW2 = np.dot(h.T, dout) \n",
    "    gradb2 = np.sum(dout, 0)  \n",
    "    gradW1 = np.dot(data.T, dh)\n",
    "    gradb1 = np.sum(dh, 0)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using \n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print(\"Running sanity check...\")\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in range(N):\n",
    "        labels[i,random.randint(0,dimensions[2]-1)] = 1  # one-hot labels\n",
    "    \n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params: forward_backward_prop(data, labels, params,\n",
    "        dimensions), params)\n",
    "\n",
    "\n",
    "def your_sanity_checks(): \n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q2_neural.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print(\"Running your sanity checks...\")\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n",
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.166109001533979, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(5.8426836294645668, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.33855778, -1.24138626,  0.42784661],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.11265089,  0.05169237,  0.39321163],\n",
      "       [ 0.07307589, -0.0335325 , -0.25507379],\n",
      "       [-0.11382109,  0.05222934,  0.39729628],\n",
      "       [-0.10534204,  0.04833854,  0.36769985],\n",
      "       [-0.32248118,  0.14797767,  1.1256312 ]]))\n",
      "(5.5798856283496789, array([[ 0.3741715 , -0.234476  , -1.36551259],\n",
      "       [ 0.35927914, -0.11439876, -0.98756037],\n",
      "       [ 0.17201142, -0.11892354, -0.53014219],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.841774  ,  0.39105083, -0.47861909],\n",
      "       [-0.02845097, -0.1067265 ,  0.02802426],\n",
      "       [-0.31375535, -0.06447558,  0.1492707 ],\n",
      "       [-0.10632801, -0.14957598,  0.03188348],\n",
      "       [-0.39323966, -0.07027277,  0.26944066]]))\n",
      "(13.553159353054369, array([[-1.24351905, -0.63487148, -1.77651934],\n",
      "       [-0.11519546, -0.20957561, -1.19656513],\n",
      "       [-0.49664769, -0.54678994, -0.4506037 ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.14837703,  0.31110522, -0.10079555],\n",
      "       [-0.41921686, -0.2662653 , -0.03680553],\n",
      "       [-0.99478518, -0.16530513,  0.51468244],\n",
      "       [-0.74542585, -0.2600954 ,  0.36736716],\n",
      "       [-1.0780784 , -0.17428871,  0.67252118]]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each row of a matrix to have unit length\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    x_sum = np.sqrt(np.sum(x**2, 1))\n",
    "    x /= np.reshape(x_sum, (-1, 1)) + 1e-20\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print(\"Testing normalizeRows...\")\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]])) \n",
    "    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    print(x)\n",
    "    assert (x.all() == np.array([[0.6, 0.8], [0.4472, 0.8944]]).all())\n",
    "    print(\"\")\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    \n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, assuming the softmax prediction function and cross      \n",
    "    # entropy loss.                                                   \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in \n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word               \n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens     \n",
    "    # - dataset: needed for negative sampling, unused here.         \n",
    "    \n",
    "    # Outputs:                                                        \n",
    "    # - cost: cross entropy cost for the softmax word prediction    \n",
    "    # - gradPred: the gradient with respect to the predicted word   \n",
    "    #        vector                                                \n",
    "    # - grad: the gradient with respect to all the other word        \n",
    "    #        vectors                                               \n",
    "    \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!                                                  \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    N = outputVectors.shape[0]                     # n_words: vocab size\n",
    "    y = np.zeros(N)\n",
    "    y[target] = 1                                     # (n_words)\n",
    "    \n",
    "    score = np.dot(predicted, outputVectors.T)               # (1, n_words)\n",
    "    out = softmax(score)\n",
    "    \n",
    "    cost = np.sum(-y * np.log(out))         \n",
    "    \n",
    "    dout = out - y                            # (1, n_words)\n",
    "    gradPred = np.dot(dout, outputVectors)             # (1, dim_embed)\n",
    "    grad = np.dot(dout.T, predicted)              # (n_words, dim_embed)\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n",
    "    K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample  \n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  \n",
    "    # a random word index. \n",
    "    # \n",
    "    # Note: See test_word2vec below for dataset's initialization.\n",
    "    #                                       \n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "    \n",
    "    cost = 0.0\n",
    "    grad = np.zeros_like(outputVectors)\n",
    "    gradPred = np.zeros_like(predicted)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    a_target = sigmoid(np.dot(predicted.reshape(-1), outputVectors[target].T))\n",
    "    cost += -np.log(a_target)                                # cost for target value\n",
    "    grad[target:target+1] = (a_target - 1) * predicted        # gradient for target value\n",
    "    gradPred += (a_target - 1) * outputVectors[target]\n",
    "    \n",
    "    neg_samples = []  \n",
    "    \n",
    "    for i in range(K):\n",
    "        j = dataset.sampleTokenIdx()\n",
    "        if j == target or (j in neg_samples):\n",
    "            i -= 1           # if negative sample is same with target or already sampled, then resample.\n",
    "            continue\n",
    "        neg_samples.append(j)\n",
    "        \n",
    "        a_neg = sigmoid(-np.dot(predicted.reshape(-1), outputVectors[j].T))\n",
    "        cost += -np.log(a_neg)                                              # cost for negative sample\n",
    "        grad[j:j+1] = (1 - a_neg) * predicted                                # gradient for negative sample\n",
    "        gradPred += (1 - a_neg) * outputVectors[j] \n",
    "    \n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the skip-gram model in this function.\n",
    "\n",
    "    # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape) \n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    idx = tokens[currentWord]                 # tokens['a'] = 1\n",
    "    input_vector = inputVectors[idx:idx+1]              # (1, dim_embed)   \n",
    "    \n",
    "    for context in contextWords:\n",
    "        c, g_in, g_out = word2vecCostAndGradient(input_vector, tokens[context], outputVectors, dataset)\n",
    "        cost += c\n",
    "        gradIn[idx:idx+1, :] += g_in\n",
    "        gradOut += g_out\n",
    "\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the continuous bag-of-words model in this function.            \n",
    "    # Input/Output specifications: same as the skip-gram model        \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    #################################################################\n",
    "    # IMPLEMENTING CBOW IS EXTRA CREDIT, DERIVATIONS IN THE WRIITEN #\n",
    "    # ASSIGNMENT ARE NOT!                                           #  \n",
    "    #################################################################\n",
    "    \n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    for contextWord in contextWords:\n",
    "        idx = tokens[contextWord]               # tokens['a'] = 1\n",
    "        input_vector = inputVectors[idx:idx+1]   \n",
    "        c, g_in, g_out = word2vecCostAndGradient(input_vector, tokens[currentWord], outputVectors, dataset)\n",
    "        cost += c\n",
    "        gradIn[idx:idx+1, :] += g_in\n",
    "        gradOut += g_out\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N//2,:]\n",
    "    outputVectors = wordVectors[N//2:,:]\n",
    "    for i in range(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N//2, :] += gin / batchsize / denom\n",
    "        grad[N//2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "def test_word2vec():\n",
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print(\"==== Gradient check for skip-gram ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print(\"\\n==== Gradient check for CBOW      ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "    print(skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient))\n",
    "    print(cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "    print(cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_normalize_rows()\n",
    "    test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check: cost at convergence should be around or below 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAFoCAYAAAD6jOlyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xd0FFX7wPHv3ZYQSuglJJBCDQQkgRCQJqgoXWkGELEB\n0psoL4IRRHrxBRFQFH4iCKiogILyWum9iUqLkNCUHghkZ3fn98fCQiCBJGSTTfJ8ztlzMjP33r0z\nkOyztypd1xFCCCGEyGyG7K6AEEIIIXInCTKEEEII4Ram7K6AEEIIcT9KKSMP9sXYoeu6PbPqI9JG\nggwhhBAeTSllpAz+FMKS4UIuY1VKxUugkbUkyBBCCOHpDBTCQl3sFMSW7twJmNiChVMYAAkyspAE\nGUIIIXKGgtgokYEgw8mYqXURaSIDP4UQQgjhFhJkCCGEEMItJMgQQgghhFtIkCGEEEIIt5AgQwgh\nhBBuIUGGEEIIIdxCggwhhBBCuIUEGUIIIYRwCwkyhBBCCOEWEmQIIYQQwi0kyBBCCCGEW0iQIYQQ\nQgi3ULquZ3cd3E4pVQxoDvwNXM/e2gghhEgnE0UpQ2msmDKwi6oNI6excJ5TkOEN1vIybyAQWKvr\n+rl05dR13a0voC8QC1wDNgN17pG2NPAp8BfO7XinpZKuI/DHjTL3AE/epw5dAF1e8pKXvOSVQ19m\ndAqgUzCFl/eNNCldK3gjn9kD7iHnv7qkNwZw61bvSqnOwFSgJ7AVGAysVUpV0nX9bApZvIB/gLE3\n0qZUZn1gMfAasBroCnyllKql6/qBVKryN8CiRYuoWrVqxm/IQw0ePJjp06dndzXyDHneWU+eedby\ntOetaRonz57EUtCCyXT3x9Yn8z9h/+79TJw5McX8NpsNa4IVv+J+mM1md1c33Tzted/pjz/+oFu3\nbnDjszQ93Bpk4AwU5uq6/n8ASqneQEvgBWDSnYl1XT92Iw9KqRdTKXMA8J2u69NuHI9WSj0G9AP6\npJLnOkDVqlUJDw/P4K14Ll9f31x5X55KnnfWk2eetTzteWuaRpH4Inj5eqUYJBzYe4AJ0ycQFhaW\nav6kS0kE+Qd5ZJDhac/7HtI93MBtQYZSygxEAO/cPKfruq6UWgfUe4Ci6+FsHbndWqDtA5QphBAi\nh1r146rsroJIhTtnlxQHjMCZO86fwTn2IqNKu6FMIYQQQmQymcIqhBBCCLdw55iMszhniJS643wp\n4PQDlHs6o2UOHjwYX1/fZOeio6OJjo5+gOpkv5xe/5xGnnfWk2eeteR5Zy1Pet5LlixhyZIlyc5d\nunQpw+W5dZ0MpdRmYIuu6wNvHCvgOPBfXdcn3yfvT8AuXdeH3HH+MyCfruttbzu3Adij63qKAz+V\nUuHAjh07duSUwTVCCCFu0DSN2PjYVAd+piW/Jw/89HQ7d+4kIiICIELX9Z3pyevu2SXTgAVKqR3c\nmsLqAywAUEqNB/x0XX/uZgalVE1AAQWAEjeOrbqu/3EjybvAz0qpITinsEbjHGD6spvvRQghRDay\n2TK2jlZG84kH59YgQ9f1ZUqp4sAYnF0au4Hmuq7/eyNJaSDgjmy7cC76ARCOcyGtY0DwjTI3KaW6\nAONuvA4Bbe+xRoYQQogczGAwYDFZsF61Ys/Agp8AFpMFg0GGIWY1d7dkoOv6bGB2KteeT+Hcff8X\n6Lr+BfDFg9dOCCGEpzMajQT4BeBwODJchsFgwGg0ZmKtRFq4PcgQQgghHpTRaJQgIQeStiMhhBBC\nuIUEGUIIIYRwCwkyhBBCCOEWEmQIIYQQwi0kyBBCCCGEW0iQIYQQQgi3kCBDCCGEEG4hQYYQQggh\n3EKCDCGEEEK4hQQZQgghhHALCTKEEEII4RYSZAghhBDCLSTIEEIIIYRbSJAhhBBCCLeQIEMIIYQQ\nbmHK7goIkZPY7XYcDkd2VyMZg8GA0WjM7moIIcRdJMgQIo3sdjtxJ+Ow2qzZXZVkLCYLAX4BEmgI\nITyOBBlCpJHD4cBqs2LMb8Rk8oxfHZvNhvWqFYfDIUGGEMLjeMZfSiFyEJPJhNlszu5quNixZ3cV\nhBAiRTLwUwghhBBuIUGGEEIIIdxCggwhhBBCuIUEGUJkgg4tOxAzIiZTy9y0fhP+vv4kXE7I1HKF\nECKrSJAhhAdTSmV3FYQQIsMkyBBCCCGEW0iQIUQmsdvsvDHsDaoGVCUsKIzJb092Xfvisy9o0bgF\nlctWplbFWvR7sR/nzp5Llv9/a/9Hw/CGhJQKoVPrTsQdi8vqWxBCiEwlQYYQmWTZ4mWYzCZW/7Sa\nsZPGMu+9eSz5vyUA2Ow2ho8azrqN6/hoyUfEx8Uz+JXBrrwnT5yk57M9ad6yOT9s+IEu3bswPmZ8\ndt2KEEJkClmMS4hMUta/LDHjYwAIrhDMH7//wQfvfUB092g6d+3sShdQPoC3JrxFq6atuJZ4jXw+\n+fi/+f9HYHAgb4x9I1n+2TNmZ8etCCFEppCWDCEySXid8GTHEZERxB6JRdd19u7aS4/OPYisFknl\nspXp0LIDACfiTwBw+K/D1Kpd6678QgiRk0mQIYSbXb92na5Pd6WQbyHem/8e3/78LfM/nQ+A1epZ\nm60JIURmku4SITLJru27kh3v2LqDoJAgDh88zIXzFxgRM4IyfmUA2L1jd7K0FSpXYN136+7KL4QQ\nOZm0ZAiRSU7En2DMyDEcOXSEr5Z/xcfzPualPi9RNqAsFouFj+Z8xPG/j/P9t9/z7uR3k+V99oVn\niT0Sy9uj3ubIoSOsWLaC5YuXZ9OdCCFE5pAgQ4hMoJSiwzMduH7tOq2atuKNV9/g5T4v0+W5LhQt\nVpTpc6az+qvVPFL3EWbPmM3ocaOT5S/rX5Z5n8xj7eq1PN7gcT5d8Cmvv/l6Nt2NEEJkDqXrenbX\nwe2UUuHAjh07dhAeHn7f9EKkRNM0YuNj8fL18pit3jVNI+lSEkH+QR5TJyFE7rJz504iIiIAInRd\n35mevNKSIYQQQgi3kCBDCCGEEG4hQYYQQggh3EKCDCGEEEK4hayTIUQOZbfb0TQNzaahaVp2VydN\nDAYDRqMxu6shhMgiEmQIkQPZ7XZOnjlJ4rVErAlW0MkRs0ssJgsBfgESaAiRR0iQIUQ62Wy27K4C\nmqaReC0RvMCCxaOm1abGZrNhvWrF4XBIkCFEHiFBhhBpZDAYsJgsWK9asWPP1rpoNg1rghULFny8\nfPDy8soRH9zZ/dyEEFlLggwh0shoNBLgF4DD4cjuqjjHYOjg5euVYwIMIUTeI0GGEOlgNBo95gPd\nbDZjNps9pj5CCHEnmcIqhBBCCLeQIEMIkWmiwqKY//787K6GEMJDSJAhhBBCCLeQIEMIIYQQbiFB\nhhC50NUrV+n3Yj8qlqlI7aq1+WjOR3Ro2YGYETEAXLp4iQE9B1CtXDUqlK7As+2fJfZIbLIyVn+9\nmqZ1mxJcIpiosCjmzpqb7Pq5s+d4rtNzhJQKoX6N+qxYtiKrbk8IkUNIkCFELhQzIoYd23awcNlC\nFq9YzKb1m/h97++u64N6D2L/nv0sWL6Alf9bia7rdO/YHbvduY7F3l17eaXHK7Tr2I7/bf4fQ0cM\nZfLbk1m+ePmtMnoN4vSp03zx7RfM+2QeCz9cyLmz57L8XoUQnkumsAqRy1y9cpXPl3zO7I9nU79h\nfQCmzZ5GeOVwAGKPxPLDdz/wzbpvCK/jPDfzw5nUCa3DmlVraNm2JR+89wENmzRkwLABAASFBHHw\nz4PM+e8cOnbpyJFDR/hp3U9898t3hD0UBsCU96bQpHaTrL9hIYTHkpYMIXKZY38fw2azUTO8putc\nwUIFCakYAsChg4cwm83Uql3Ldb1I0SKEVAzh0F+HXGlqR9VOVm6dqDrEHolF13UOHzyM2Wx2BRgA\nFSpWwNfX1523JoTIYSTIEEKkm1Iqu6sghMgBJMgQIpcpH1gek8nEnp17XOcuX7rM0cNHAahYqSKa\nprFz207X9fPnznPk0BEqV63sSrN98/Zk5W7dtJXgCsEopQipGILNZmPvrr2u64cPHebSpUvuvDUh\nRA4jQYYQuUz+Avnp2KUjY0eOZeNvG/nrj78Y1n8YRqMRpRRBIUE0b9mc4QOGs23zNn7f9zsDXh6A\nX1k/Hm/xOAC9+vdi/S/rmTFpBkcPH2XZp8tY8MECeg/oDUBIxRCaNGvC8IHD2bV9F3t37WV4/+Hk\n88mXnbcuhPAwEmQIkQvFjI8hom4EPTr3oEu7LkRGRRJSMQQvby/AORA07KEwenTqQbvH26EMiv9b\n/n+ufVCq16zOnIVzWPnlSh6t9yjTJkxj+BvD6RDdwfUe0+dMp3SZ0nRo2YGe3XvS7fluFC9RPFvu\nVwjhmZSu69ldB7dTSoUDO3bs2EF4eHh2V0eIB6ZpGrHxsXj5emE2m++b/lriNSKqRPDmO2/SuVvn\nLKjh3TRNI+lSEkH+QWmqsxDCM+zcuZOIiAiACF3Xd94v/e1kCqsQudD+vfs5cvAID0U8xOVLl5k+\ncToK5eoOEUKIrCBBhhC51JyZczh6+Chms5kaD9VgxfcrKFK0SHZXSwiRh0iQIUQuVL1Gdb775bvs\nroYQIo+TgZ9CCCGEcAsJMoQQQgjhFhJkCCGEEMItZEyGEDmYzWbL7iqkWU6qqxAic0iQIUQOZDAY\nsJgsWK9asWPP7uqkmcVkwWCQBlQh8goJMoTIgYxGIwF+ATgcjuyuSroYDAbXqqJCiNxPggwhciij\n0Sgf2EIIjybtlkIIIYRwCwkyhBBCCOEW0l0ihLgnu92e6WM/ZGyGEHmDBBlCiFTZ7XbiTsZhtVkz\ntVyLyUKAX4AEGkLkcm4PMpRSfYFhQGlgD9Bf1/Vt90jfBJgKVAOOA+N0XV942/XngI8BHVA3Tl/X\ndd3HLTcgRB7mcDiw2qwY8xsxmTLnz4XNZsN61YrD4ZAgQ4hczq1BhlKqM86AoSewFRgMrFVKVdJ1\n/WwK6QOBVcBsoAvwKPChUuqkrus/3Jb0ElCJW0GG7q57EEKAyWTCbDZnWnk5aW0PIUTGuXvg52Bg\nrq7r/6fr+p9AbyAReCGV9K8AR3VdH67r+l+6rr8HfH6jnNvpuq7/q+v6Pzde/7rtDoQQQgiRIW4L\nMpRSZiAC+N/Nc7qu68A6oF4q2aJuXL/d2hTSF1BK/a2UOq6U+kopFZpJ1RZCCCFEJnFnS0ZxwAic\nueP8GZzjM1JSOpX0hZRSXjeO/8LZEtIG6IrzHjYqpfwyo9JCCCGEyBw5bnaJruubgc03j5VSm4A/\ngF7Am/fKO3jwYHx9fZOdi46OJjo62g01FUIIIXKWJUuWsGTJkmTnLl26lOHy3BlknAXsQKk7zpcC\nTqeS53Qq6S/rup6UUgZd121KqV1AhftVaPr06YSHh98vmRBCCJEnpfTFe+fOnURERGSoPLd1l+i6\nrgE7gGY3zyml1I3jjalk23R7+hsev3E+RUopAxAGnHqQ+gohhBAic7l7dsk04GWlVHelVBVgDuAD\nLABQSo1XSi28Lf0cIFgpNVEpVVkp1QfocKMcbuQZpZR6TCkVpJSqBXwKlAM+dPO9CCGEECId3Dom\nQ9f1ZUqp4sAYnN0eu4Hmt005LQ0E3Jb+b6VUS2A6MACIB17Udf32GSdFgHk38l7A2VpS78YUWSGE\nEEJ4CLcP/NR1fTbOxbVSuvZ8Cud+xTn1NbXyhgBDMq2CQogMWbFsBa8Neg0ApRS9+/fm/f++7zr+\naetP+JWVSV9C5GU5bnaJEMIzNG/ZnPA6twZSe+fz5unOT7uOS5dJbaa6ECKvkCAjG7hjV8ucRHbg\nzB188vtQPqh8dldDCOHBJMjIYu7a1TInkR04hRAib5AgI4u5Y1fLnER24BRCiLwj733KeYjM3tUy\nJ5EdOIUQIm9w9zoZQgghhMijJMgQQgghhFtIkCGEEEIIt5Agw8N0aNmBmBEx6coTfzwef19/Duw/\nAMCm9Zvw9/Un4XICAMs+XUZouVBX+mnjp9G8YfNMq7MQQgiREgkycoGyAWXZfXg3VUKruM4596K7\nRXHr+JWBr7D0m6WZWoc7AxshhBAi188uUUoZuXGfmqahaVq21udmHQxayvGdw+HAYXfcVc/UFvDS\nNA2z2YxPfh+uX78OwPXr13E4HCQmJmI0GbFara7jmyxelmTH8GCLZN2sr9Vqveczvnn/WfnvIIt/\nCSFE9sjVQYZSykgZ/EmiDAlw8uxJisQXydY6aTaN+DPxWBItKa6TkeRI4uLViwwZOIS1q9diMplo\n074Nrdq3wuawMaDnAJo82oTTJ0+zY+sO6tSrQ/vO7RnUaxDjpo8jMDCQP2L/gPyw4/cd+Pj4cOTE\nEWwWG1v3bAXgi8++YNvWbUyYNgGAo4eOsvTTpfx96G8cNgeVqlSi/9D+VKxS0VWvRhGNGD5qOJt+\n28TWzVspXqI4fYf0pUGjBpw+eZrojtFggRpVaoCCJ1s9yYiYEXfdn81mw5pgBQVmU9ZM4ZXFv4QQ\nInvk6iADMFAICwo7VrAUtODl65W9FdIMWK5a8CrohdF894eewdvAt99/S9v2bVnw9QL+2P8H494c\nR75S+Xi87eMYChv49udveabrMzw76FlnHoMBCoJPKR98yvjg/Y83FACf0j745PfBq6QX+IJPGR8A\nzMXMmHxNrmP9tE7TDk0JCQiheLHiLFu0jFdffZUvv/sSbx9vZ8UssPCThfQf0p9BowexdNFSxo4Z\ny8p1KwmoFMDE9yfy2qDX+Pzbz8mfPz9eXl5YCljuuj+j5rxnr0JeWbJOiCz+lTlsNptHliWE8Gy5\nPchwMmBHec4CWCazyfW6kzIo/Pz8ePWNVwEIrhDM4YOH+e7r72j9TGsMykB47XA6P9vZlefM6TOg\nnF0g+fLlw8vbCwzg7e1Nvnz5MJvNGAwG8uXLB+A8Nt06jqwfiU1ztjD4lfbjjbFv0KROE/bs2kOD\nJg1uVAzaPt2WJ9s8CcDAVwey7NNl/PXHX0Q1iKJIsSIopShRsgQFChZI9d4VCrvZjtlszrJ/C1n8\nK+MMBgMWkwXrVWumPkeLyeIMjoUQuVreCDJymOoPVU92HPZQGJ8u/hR05/Ht3RiZ4cKFC3w09yP2\nbd5HwvkEHA4HSdeTOH3qdLJ0FSpXcP3snc+bAgULcP7c+Uyti/AsRqORAL+ATN/QT8bJCJE3SJCR\nA3l7e2dqeVPenkLCpQRe7vMy1UKrkS9fPnp07nHX4Mw7x5AopfL0brJ5hdFolIBACJEh0l7pgfbv\n2Z/seN/uffj5+YFKJcMD+n3/77Ru35patWsRGByIyWziwvkL6SrjZgAiQYcQQoibJMjwQKdPnWb6\nhOkciz3GmlVr+OS/n1DOr1z6CtHTnrSQTyGmjZ7GibgT7N+zn1GvjsI7X/paS/zK+qGU4tcff+Xi\nhYtcS7x2z/TxcckXEEuJrL0hhBA5mwQZHkYpRau2rUi6nkT3jt2ZNHYSz7zyDP1H9L+RILWM9zm+\nh5HjRhIUGsTQfkMZM3IM0d2jKVqs6F31uvstb50rUaoEvQb0YubUmTxW/zEmjZ103/dNqcyMpBFC\nCOGZlK6n4ytvDqOUMlOZQIxUII5v16xZQ1hYWLbWSdM0jp86jldBrxRnl6TEptk4ceYE5oJmt60t\ncfvsEnfO+rBpNpISkjDYDDSs1ZDvN3xPaPXQFNNuWr+JTq06ceD4AQoWKpih99M0jaRLSQT5B3nE\nzCIhhMhpdu7cSUREBECErus705M3T7dk/LzuZ55q/hSh5UKpHlid5zo9x7HYY8Ct/UC+W/kdHVt1\npELpCjz28GPs2LrDlf/mniC//O8XmtRpQiW/SnR7uhv//vOvK42u60yfMJ3aVWsTXCKYFk1asHXD\nVtf13p16M2lk8m/9F85dICowim0btgHQ7uF2rF662nX9yUpPsmb5Gsb0GUPbGm158bEXmTxiMj1a\n9aBNvTb0faYv86fN58XHXqR1tdY8UeUJ5k+dzxMVn6BNVBuG9BhC/LF4V3nfr/yeTs06JavD8oXL\naVu/LVGBUbRv1J5vv/jWdW3MkDEM6j4oWXqbzcZjNR7jm8++AWDjzxt5sd2LNKnahKbVmjKo+6Bk\n73nTob8O0faxtoSUDKFZVDM2b9h8z3+zrZu28vQTTxNSKoTIapGMHj76vl0zQgghskeeDjISExPp\n1b8Xa35dw7JVyzAajbzY9cVkaSaNnUSfgX34YcMPBFcIpt9L/ZINbryWeI25M+cy68NZfLnmS07E\nn2DMyDGu6x+89wEfzP6AN995k3Wb1tHokUaM6DfC9YHbLroda75ak2wmx7dffEvJMiWp83CdVOu+\neNZimrRqwpxVcyhUrBA/fvkjLw9+mXlfzOORJx9h+fvLqVSzEoMmDgIzrPhoBUopJn84GYPJwPS3\npicr7/ZuiR+/+5Gpb06l+yvdWf7Tcp7u9jQxg2PYsckZYLXr0o5Nv2zi3L/nXHl+/eFXkq4n8Xjb\nxwG4nnidbr268enaT5m7fC4Go4FhLw676z7GjR7HKwNfYe2GtURERtCjcw8uXriY4j3/ffRvurXv\nRqt2rfhx84+8//H7bNuyjTdefSPV5ySEECL75Okgo0WbFjzR6gnKBZYjtHooU2ZN4c/f/+Tgnwdd\naXoP7M0jjz1CUEgQw/4zjPjj8cQeiXVdt9lsTHx3ItVrVqd6jer06NmDDb9scF2fO2sufQf3pfVT\nrQmuEMzrb75OhSoV+Gz+ZwA0bdEUgF/W/uLKs2rZKtp0bnPPuj/W/jEatWhE8TLFOXr8KOjgbfGm\ntF9pLv9zmQJFCoAXlChTAszQqGUjlFL4l/en8/OdObDnQKr7hyyas4g2z7Sh/bPtCQgKoGvPrjR9\nsimfvP8JADVq16B8cHlWf36rdWXV0lU82upR14DRpi2a8siTj1C2XFkqhlZk1JRRHP7jMEcPHk32\nXi/0eoEnWj1BhYoVGD99PAULFeSzTz5LsV7vTX+Ppzs/zQu9X6B8UHkiIiN4a8JbLF+8HKvVes/n\nJYQQIuvl6SAj9kgsfV/oS/0a9aniX4WosCiUUpyIO+FKUzW0quvnkqVKous6587e+gafzycfAeUD\nXMelSpfi7L9nAbiScIUzp85Qu27tZO8bViuMvw//DThX6WzRvoWrm+GPvX9w5OARWnVsdc+6B1UK\nAuBk3EmSriehoxMzKIZ2D7fj84WfczXxKqfiTwHOVoraDW7VoWhx56DOi+dTaTE4/Dc1a9dMdq5m\nnZrEHr4VXLXr0o6VS1cCcO7fc2z4aQNto9u6rsfFxvGfPv+hTb02NKrciDZRbVBKcebkmWTlhtcJ\nd/1sNBqpWasmh/46lGK9Duw7wPJPl1PJr5Lr1fXprgAcP3b8Hk9LCCFEdsjTi3E91+k5ypUvx+RZ\nkylVphQOu4OmdZsm+4Z/++DMm10Kt3eX3DkQUylFegfTtuvSji6Pd+Hf0/+yculK6jxch9JlS98z\nz819T26OR8jnk48ufbrwcPOHmTV6FvkL5qfXyF6cjDsJgMF4dzz5IGtatOzQklnjZ7Fv5z52b91N\n2XJlqVnnVmAysPtAypYry6gpoyhRqgS6rtOxSccH2n316tWrdH2+Ky+98tJdz7hsQNkMl5tdUttZ\nNzPJyppCiOyUZ4OMC+cvcPTwUaa+N5U6Uc6xD1s3bb1PrvQpULAApcqUYtvmbdStX9d1ft+ufYRF\n3JrlUqFKBUJrhvLloi9Z89UaXh//eprfo3xweSxeFhxXHRQpWgQ/fz8qhFZg+y/bKV6yuCvIOLz/\ncJrLDKwQyJ5te2jZoaXr3J5tewiuGOw69i3iS5PmTfjms2/Yu2Nvsu6dSxcucfzocUZPHc1DkQ8B\nsGvLrhTfa+e2nUTWiwScH7p7d+/lhd4vpJg2rGYYh/48RLnAdK4Z4oHsdjtxJ+Ow2tzbzSM70Aoh\nslOeDTIKFylMkaJFWPTxIkqULEF8XDwTYiZk+roMrwx4hWkTplEusBzVwqqxeOFijvx1hJj/xmDT\nbu1G2bpja6a8OYV8Pvlo2Kxhsmvg/FCya3bX2hQOuwObzYbZYuapLk+xdOZS9u/cT5XwKlQJr8IX\nH37BqJ6jaNSyEbpV56eVPznLcdix2+3ouo7dZsdms6HbddCd40vsmp2uPbvyRr83qFClAnUa1OG3\nH37jpzU/MWvxrGT1atWpFUNfGIrD4eDJp550XfPJ74NvEV+++OQLChctzOn408yeNBulFHZH8k22\nFnywgMDgQCpUrsC8WfO4fOkynbve2vzt9haLPoP70ObRNrwx7A2in4vGx8eHg38e5LeffuPtKW9n\n0r9Y1nA4HFhtVoz5jXct155ZZAdaIVKXFS2JaZHbWxvzbJChlOL9Be8zevhomtVrRkjFEMZOGkuH\nFh1cC1mluABVOoOQF195kYSEBMa+MZZz/56jYuWKTH5/MmVKlCEpIcmVrnHTxkx/azqPPvkojiQH\nSUm3rqGD3WrHesWKw+xAKYV2TcN6xfktuPOznVkxbwVbftvC/77/H/kL5Kd89fLEHohl94bdYIc2\nXduwcMZCdKuOlqg5y0h0lqElaei6jnZFI8kniah6UfR/rT+L5i5i+lvTKV22NCPeHkFo1dBkda5Z\nsyZFixcluGIwBfIVSHbtzclv8t/x/6XL410oF1iOASMGMPD5gTiSHFhMFhx2532MiBnBrGmz+GP/\nHwQGB7Jg6QKKFC2S4vOuWq0qX3z7BRPHTKT9k+3RdZ3yQeVp8/S9B8l6MnfvDCw70Apxt6xqSUyL\n3N7aKItxZYOUIui443E0qd2ElT+uvGtxKpvNxvFTx7EUtGR4Ma6PZ37MV0u+4uuNX991bcXyFXw0\n5yM+W/4ZAWUC0vyhl3g1kbrV6zL1vak83uLxNNclKyN3T12MS9M0YuNj8fL1clu9PPXehchuN3//\n3NmSmBY2mw37VbvH/44+yGJcebYlIzvdvqulzWbj/LnzTB8/nYjICGrWqpliHpPJhNlkTvMqocsX\nLqfaQ9XwLeLL7q27+fSDT3nmhWfuyn/61Gk2rd9EcIVg17fq+/1nvznDZu7MuRQuUpgnWz+JwZCn\nJyoJIXKMoTs9AAAgAElEQVQgd7ckpkVub22UICObbdu8jY4tO1KhUgXm/t/cTCs3LjaO+e/O5/LF\ny5QuW5rur3SnR78ed6Xr9nQ3SpUpxcgxI9Nc9om4E0SFReHn78eMOTMkwHCDtavXYrVaaf1U6zSd\nF0IITyRBRjar16Ae8ZfuXm77QQ2JGcKQmCH3Tbdu0zrA2XxovZy2/kn/cv5uqbO4JSIygnaPt8O3\nsC/1GtRzna9RqwYdW3a863xKNE1zvXL74DIhhGeSIEMID1SkaBEmzJzA60NfZ/qc6RQtemtX3Lff\nfZv/vPqfu87fyWZzbnqHgvze+XP14DIhhGeSICMPsdvt6I6UB/raNed01gdZLCur5eZv5w6HgzIB\nZVi0ahFGU/J7DAkLYcmaJfctw6g585l8TFitMpVViIzatH4THVt25I+4PzK8I3ReJUFGHmG32zlz\n9gyaLeUgwmazoV12Xkvr4NLsZjFZ8Cvll6s/OI0mY4YHpikUdrMdo8mIw5r96wEIkVN0aNmB6jWq\nEzM+xnUus9dQyityxqfJg3JgvLnYVE76pn6TpmnYNBv6dR2TlrF/Ms2mkZCQgNHbmOKHsm7UMXgb\nnNfNnv+hbbPbSExMJCkp6Z4fwjabLdVrQggh3Cu3BxkOLmMlCSNJYE2wknQp6f65PIzdYUe/pnMt\n4VqGy7DZbST+m4g5vznVeeEmgwlHkgO7zfOnVNltdue/Z/4kHKZ7f0u3mCwyA0YIkSaDXxnM5vWb\n2bJhCx/O/hClFFPfmwrAnl17eGf0Oxz86yDVwqox/f3pBFe4td3C2tVrmT5xOof+PETpMqXpEN2B\ngcMH5um/P7k6yNB13a6UigdKAfgV9yPIPyiba5UxQWWDHmgJ3LQs/pSTxjhomnN10qCy91/EJifd\nlxAie42ZOIajh49SpVoVhr8xHF3X+fPAn+i6zqSxk4iZEEPRYkV5beBrDO07lBVrVwCwZeMWBvUe\nxNuT36Zu/brEHo3ltQGvgYLBrw3O5rvKPrk6yABXoGED0rTQlKd60HprmoaPjw9ePu5bYTKrOcyO\nHP1vKoTwPAULFcRsMZMvXz6KFS8GOBdQVErx+puvuzZ07Du4L891eg6r1YrFYmH6xOn0G9KP9s+0\nB5xT/YeNHMa40eMkyBBC5Cwxg2LwK+dHzyE96dmhJ206t6FVx1bZXS0hcrUqoVVcP5csXRKAs/+e\nxa+sHwf2HWD7lu28O/ldVxq73Y5m1bh+/Tre3t5ZXl9PIEGGEEIIkQa3t5renG1yc1mAq1ev8urI\nV3my9ZN35curAQZIkCE8jMxHF0JkN4vZuVN0eoTVDOPIoSOUDyrvplrlTBJkCI+i6zpKKXLz7sCZ\nTebvC5G5/Mv7s3P7TuKPx+OT3weHw5Hi36Tbzw1+bTA9OvfAr6wfLdu1xGAw8Pu+3/nrwF8MHzU8\nK6vvUfLuvBqRzKqvVvFovUcJKRVC9cDqRLeN5sD+AwQUDuD8ufMAXLxwEX9ff/q+0NeVb8akGTz9\nxNOu4z8P/Mmz7Z+lkl8lHqrwEAN6DnDlB+cv5cypM6lXox4hpUJ4vMHjrP56NQDxx+Pp1KoTAKHl\nQgkoHMCQPvfffyUvipkRQ88hPQGYu3yujMcQIhP17t8bo9FIk8gm1AypyYm4EykG87efa9ysMQuX\nLeTXn36l5SMtafNoGz6c/SEB5QOysuoeJ1e2ZCiljCQPoExwa8OovEjTNOyOlNe/+OfMP/R7sR+j\n3h7FE62e4MqVK2zZuIXygeUpWqwomzdspkWbFmzdtNV1fNOWDVtcG3VdvnSZzq0707VHV96a+BbX\nrl3jndHv0LtHb5atXAbAf6f8l6+Wf8XEdycSFBzE5g2bGdBzAMVLFCeyXiQfLPqAns/2ZP2u9RQo\nWCBP92UKIbJHcIVgvv7h62TnOnXtlOy4Wlg14i7GJTvXqGkjGjVt5Pb65SS5LshQShkpgz+FsLhO\n/ksZEuDk2ZMUiS+SjbXLPpqmcebcGQILBd415fOf0/9gt9t5ovUTlPUvC0DlqpUBiKwfyabfNtGi\nTQs2/raRZ559hsULF7v6Hrdv3U7fIc6WjY/nfUxYzbBkTYOTZ00mMjSS2COxlA0oy6xps1j6zVLC\n64QDEFA+gK2btrLoo0XUrV+XwkUKA1CseDEZkyGEEDlcrgsyAAOFsFAXOwVxrin9E1asYClowcvX\nK02F2O32B1r8yt3Su8CU45oD7YyW4j2FhoXSoHEDmkU1o3GzxjRu2piWbVs6txN/uB6LFy4GYPOG\nzYx4cwRHDx9l0/pNXDh/AbvNTu26tQE4sP8AG37dQCW/SsnKV0pxLPYYmqZxLfEa0e2ik/Vl2jQb\n1WtWz8hjEEII4cFyY5DhVBAbJW4EGSbsKDCZTGlauMlut3P639NYbVZ31zLD0rs52L26iQwGA0u+\nXsL2Ldv59cdf+WjuR0waO4lVP66iXsN6xIyIIfZILIf+OkRkvUgO/XWIjb9u5OKFi9SoVcPVpZF4\nJZHHWjzGG2PeuGuQVMnSJfnz9z8B+OTzTyhVulTy+/GyIIQQInfJvUHGA3A4HFhtVow+RkxGz3tE\nNrsNa2Lmb91du25tatetzaDXBhFZLZLvVn7Hy31fppBvId6d/C7VwqqRzycf9RrWY/aM2Vy6eMk1\nHgOges3qfLfyO/zL+ae4Vn+lKpXw8vIi/ni8a9W8O90MAu12z98/RQghxL153ieoBzEZTR677bmd\nzPsQ3rV9F+t/WU/jpo0pVqIYO7ft5MK5C1Sq4uz2iHo4ihXLVtB7YG8AQquHkpSUxIZfN9Crfy9X\nOT169mDJ/y3hledfoc/APhQuUpjYI7F88+U3TH1vKvkL5KdX/17EjIjBbrcTWS+ShMsJbNu8jYKF\nCtIhugP+5fxRSvHDdz/QrHkzvL298cnvk2n36mlS2yVW0zRsNhtGzYgiY1NUbXbZgVYIkb088xNU\nZKmChQqyZcMW5r8/n4SEBPwD/Bn9zmgaN2sMOIOMtavXUr9BfcA5xiLq4Sh++uEn6kTVcZVTqnQp\nvvr+K8a9OY4uT3XBarXiH+BPk0ebuKZ6DR81nOIlivPe9Pd4beBrFPItRFjNMPoP6w9A6TKlGfqf\noYyPGc/QvkPpEN2BabOnZfETcT+DwYDFZMF61ZpiwKhpGtYEZ3ed3ZzxgPLmDrQOPHd8kRAi91K5\nbdEjpZSZygTyKEmuMRlfUp1Y1q5Zs4awsLD7lqFpGsdPHceroFeWtGTEDIrhSsIVpsyfkqb0Ns1G\nUkIS5cqUS/PmYImJiRw5eISQSiH4+OT8lgFN00i6lESQ//13YfVU9xpcrGkasSdi8Sr0YBvaGQwG\nHA5Hjn9WQmSmm7tSG/MbMZmy77u2zWbDftXu8b+bO3fuJCIiAiBC1/Wd6ckrLRke4NW3X5UVLvMg\no9F4zzE1ZpM5U3aZ9eRZUkJkh/u1JGalm62NuZUEGR4gf4H8yY5PnThFq6atWPL1Ete4CCGEEJnD\naDQS4BfgEQF4epcjyGlyb/iUip/X/cxTzZ8itFwo1QOr81yn5zgWewxwLmvt7+vP6q9X0++5fjSq\n3IjuLbpz/Ohxft/9O88++SwNKzZkQLcBXDx/0VXmgT0H6PNMH5pVb0bjKo3p2b4nf+7703V95bKV\n1C5bmzr+dahdtrbrNW/aPMDZXTLsxWGu9CN6j4AkWDxvMU2rNeXxhx5n3tR5ye7jeOxxOrToQEjJ\nEJpFNWPjbxvx9/Xn+2+/d+fjE0KIXMFoNLpaCrPzlZsDDMiDQUZiYiK9+vdiza9rWLZqGUajkRe7\nvpgszYyJM+jRuweffPsJRqORkX1HMvOdmQx/ezjzv5pP3N9xzJk8x5X+6pWrtO7Umo+/+ZiFqxZS\nLrgcA54dwLXEawA0b9uc7/d8z9rda/l+z/e8M/sdTGYTD0U+lHpFreCdz5uFqxcy8I2BfDD9A7b+\nthVwNn//Z8B/yF8gP6t/Xs2EdyfwzpvvyEZZQgghPEqe6y5p0aZFsuMps6ZQI7gGB/886BoQ2bNf\nT2rXq41XQS+iX4pmZN+RzFk+h7AI56DRts+0ZdXyVSz8YCFfLvuSf07/Q7HixXj6mad5odcLdHyh\nI98s/YYm4U3I75ufps2bMvT1oeTzyUfc33FM+M8EajeszZsj3+Ti+Yt4K2+CgoKSV9QITz/7NAGB\nAQQEBrD046VsXb+VyIaRbPl1C6fiT7FizQpKlykNwGujXyO6bbT7H6DIUqlNcc3qMoQQIiPyXEtG\n7JFY+r7Ql/o16lPFvwpRYVEopTgRd8KVpkpoFdfPRUsUBSCkckiycyfjTrLww4X06teLDxd9SMXA\niiyZvYTGlRvT9dGuOOwOnn/5eSb9dxJbN2xl0thJXEm4wuDnBuMf5M/+P/cz5PUhLF25lJKlSrJ3\n917ijt222c4d/zLFSxbn/FnnbqbHY49TsnRJihUv5rpeK6JWZj4mkc1uDkyzX7WTdCnpgV72q/Zc\nP7hMCOGZ8lxLxnOdnqNc+XJMnjWZUmVK4bA7aFq3abJlt2+ftnqzC+L2c5pVIykpideHv06Lti3o\n16Ufly9d5q1332L/3v0sXrgYo9WIr68vtevW5rXRrzGw10BOHj5JwUIFOXnxJD1e7sFjTz4GQNVq\nVfnn1D8sWbiE4aNvbS52O6UUuuPBZ6DYbfZcsRNtbv92ntkD03L74DIhhGfKU0FGwuUEjh4+ytT3\nproWkdq6aWu6y/n3n39Bx1XG3u17eX3C69RvUp/NmzcTVCGIfev3udLXDK+Jfk3nyF9HWLByAW0f\nb0vN8JrJyixUuBBHjx5N0/uXDy7PP6f/4dzZc67ukl07dt0zj8FgwGK0YEu0kWRNSs/teqzc/u38\nflNchRDC0+WpIKNAwQIUKVqERR8vokTJEsTHxTMhZsJ9B0zeuYbFnesWBAQF8O3n31I1rCpnT5/l\n6L6jeOfzdl3/9otvQYNn+zzrLMsBly5c4lriNfL55Ev3fUQ2jKSMfxmG9BnCqLdHcSXhCpPGTnLe\nRyq3YjQaKV2yNOX9ynv0oi/pId/OhRDCs+Xer4EpUErx/oL32bd7H83qNWPMyDGMGjfqxsVbaVLK\nd7uixYuCutUK8ua0N7l86TLdnujG9p+34zA6KFKsiCv9T2t+AmDW27N4usHTcBWG9RjGJ3M+caW5\nfPEyISEht97vHnGPwWBg/MzxXEu8RqtHWjF8wHAGvjoQXdfx9vJONZ+nTNmSqV9CCJE35KmWDIAG\njRvw45Yfk52LuxiX7Oeby4oDRNSLYFv8tmTpn+ryFGfPn+Xdye9iMpmoGV6TV8e9ytHDR2neqjnt\nHmtHaHgokY0j2bZ5GyfPnqRtj7aMfmc0AIsXLmberHkEVgnkWOwxivkX47q6zjPdnwHgnfffoVXT\nVsnec+pHU5MdBwQGsGzVMlerxLbN21BKERgc+OAPSQghhMgEeS7IyCwv93sZo9nInJlzOPvPWYqX\nKE77Z9rj7e3Nex+9x+Rxk+nesTve3t40a96MIa8PceWN7h7N1StXmTFpBhfOXSCoQhDT50wnoHyA\nK839unB++99vlPcvT8XKFTl65Cgxr8cQWS+ScoHl3HbPQgghRHrIBmkpyOoN0tLLptn45tNvWDJ/\nCSdPnqRosaI0eqQRo94eReEihVPMkxs2FBNCCJH1ZIO0PKh5m+a83OtlCRiy2b12Uk0PGcQqhMiN\nJMgQIoPsdjtxJ+Ow2qwPXJbFZCHAL0ACDSFEriJBhhAZ5HA4sNqsGPMbMZky/qtks9mwXrXicDgk\nyBBC5CoSZAjxgEwm0wN3W9mxZ1Jt8rbM6r5KD+nqEiJ1uTfISLjt3mwY0Z3fGNOypLamadg0G0bN\niI7nDYy1a3ZsWtru5abcvgy3EJnZfZUe0tUlROpyY5Dh4DJWtmABnL/1/2IhCawJVpIu3X9Jbc2m\nYU1w/qF6kGZwd7HZbM57yZ+Ew5T2b225fRlukbdlVvdVekhXlxD35vbfRKVUX2AYUBrYA/TXdX3b\nPdI3AaYC1YDjwDhd1xfekaYjMAYIBA4Cr+u6/h2Arut2pVQ8p5KtZloEwK+4H0H+d2ypngJN00An\nS/9YpYfNZsPuYyeobPqmo0qzrsgLMqP7Kj2kq0uI1Ln1E1Qp1RlnwNAT2AoMBtYqpSrpun42hfSB\nwCpgNtAFeBT4UCl1Utf1H26kqQ8sBl4DVgNdga+UUrV0XT8AzkADbv3mK6VsgGs56vsxGAzkz5cf\nq9WKw5q1/btpYcCAdz5vvLy8JGgQQgjhsdz9NX0wMFfX9f8DUEr1BloCLwCTUkj/CnBU1/Wb+53/\npZRqcKOcH26cGwB8p+v6tBvHo5VSjwH9gD6ZUenM3mbbHaRVwrPNmjaLL5d+yfFjxynjV4ahI4bS\nrmO77K6WEEJkKbcFGUopMxABvHPznK7rulJqHVAvlWxRwLo7zq0Fpt92XA9n68idado+UIXvINts\niwexbfM23pr4FoFBgXz+2ecM7DWQiMiIZEvHCyFEbufOUYDFcQ68PHPH+TM4x2ekpHQq6Qsppbzu\nkya1MoXIcguXLaRhk4YElA/g+Z7PY7fbOX36dHZXSwghspRMNRDCzd76z1tUrV6VWhG1srsqQgiR\npdw5JuMszsGXpe44XwpI7Svd6VTSX9Z1Pek+ae77NXHw4MH4+vomOxcdHU10dPT9sgqRIUP7DmXn\ntp0sX73cI2cqCSHE7ZYsWcKSJUuSnbt06VKGy3PbXz1d1zWl1A6gGfANgHLuX94M+G8q2TYBT95x\n7vEb529Pc2cZj92RJkXTp08nPDw8TfUX4kHt3rGbpYuW8tvO3yhZqmR2V0cAP6/7mXcnv8tff/yF\nwWAgIjKCMRPHUD6oPPHH44kKi+KDRR/w0dyP2LV9F0EhQUyYPoGIyIjsrroQWSKlL9637cKabu7u\nLpkGvKyU6q6UqgLMAXyABQBKqfFKqdvXwJgDBCulJiqlKiul+gAdbpRz07vAE0qpITfSxOAcYDrL\nzfciRLr8c+YflFIEhdx/bRaRNRITE+nVvxdrfl3DslXLMBqNvNj1xWRpJo2dRJ+Bffhhww8EVwim\n30v9PHqmmRCezK3tt7quL1NKFce5cFYpYDfQXNf1f28kKQ0E3Jb+b6VUS5yzSQYA8cCLuq6vuy3N\nJqVUF2DcjdchoO3NNTKE8BRRD0fx7c/fZnc1xG1atGmR7HjKrCnUCK7BwT8P4uPjA0Dvgb155LFH\nABj2n2E0rduU2COxhFQMyfL6CpHTub2TWNf12TgX10rp2vMpnPsVZ8vEvcr8AvgiUyoohJts/G0j\n42PG88v2X7K7KuKG2COxTBk3hV3bd3H+/HkcDgdKKU7EnaBi5YoAVA2t6kpfslRJdF3n3NlzEmQI\nkQEyEk24XWbsjJkTFx+7fOkyRw8fze5qiNs81+k5ypUvx+RZkylVphQOu4OmdZsm22zQZL71Z9E5\njAzpLhEigyTIEG6VWTtj5sSdLjt17USnrp2yuxrihgvnL3D08FGmvjeVOlF1ANi6aWs210qI3E2C\nDOFWmbEzpux0KTJD4SKFKVK0CIs+XkSJkiWIj4tnQswEV2uFECLzyWJcIkvc3BkzIy9ZX0JkBqUU\n7y94n32799GsXjPGjBzDqHGjbly8lSalfEKIjJG/3gLInHETKdE0DU3TMGgZj2dvlnF7v/m95MTx\nGyJrNGjcgB+3/JjsXNzFuBR/BijkW+iuc0KItJMgQ2TauImUaDaN+DPxWBItD9ZdkmAFBWaT+b7p\nc+L4DSGEyI0kyBCZMm4iNQbNgOWqBa+CXhjNGfvQN2rOfF6FvDCb7x1kyPgNIYTwHBJkCJeb4yYy\nvVyzyfXKCIXCbra7xmjcjx17ht5HCCFE5pIgQ2S7ns/2pHJoZYaOGJrdVckQm82WrfmFEMJTSZAh\nRAYZDAYsJgvWq9YHbj2xmCwYDDLZSwiRu0iQIUQGGY1GAvwCMmVWjsyIEULkRhJkCI8WMyiGhEsJ\njJk6JrurkiKj0SjBgYfJyu4n6eoS4t4kyBAe4erlq9QuW5slPyyhYmjF7K6OyIEys/sqPaSrS4jU\nSZAhsp3u0NHRZWVF8UAys/sqPaSrS4jUSfgt7mvdmnWElgtF13UAft/3O/6+/ox/a7wrzbB+wxjQ\ncwAAq79eTdO6TQkuEUyDWg1YunBpsvJa123NhzM+ZPTA0TSu0phjfx7jm4XfABD9WDS1y9amV8de\nyfJ8tuAzIkMjqR5YnZFDR2K3yzRVcTej0Zjh5esz+pIAQ4jUSUuGuK+69ety9cpV9u/ZT9hDYWxe\nv5lixYux6bdNrjSbN2ym/9D+7Nu9j1d6vMKwkcNo/VRrtmzcwshhIylepjhtnmnjSr9o7iJeHvwy\nvYY6g4lLFy7RvUV35iyfQ3Cl4GTrYWzftJ3CRQrz2TefEX88nt49elO9ZnWiu0dn3UMQQgiRbtKS\nIe6rYKGChIaFsnH9RgA2rd/Ey31f5ve9v3Mt8RqnTp7iWOwx6tavy7xZ82jYpCEDhg0gKCSI9s+0\n56nop1g0d1GyMiMbRNK1Z1fKlivLmFFjWLd2HQC+hX0pWrwoBX0LutIW8i3E4JGDCa4QTLPmzWjW\nvBnrf16fdQ9ACCFEhkiQIdIk6uEoV8vFlo1beLL1k1SoXIGtm7ayZcMWSpUpRWBwIIcOHqJ2VO1k\necNqhRH3d5yruwWgSo0qrp9Pxp/k4sWLqb53cKXgZOM1SpYqydmzZzPr1oQQQriJdJeINKnXsB7L\nFi3j932/Y7FYCKkYQtTDUWz8bSMXL16k3sP10lVevnz5XD+v/N9KTsWfco3LuNOd+6kopbJ8cJ8Q\nQoj0k5YMkSZ169UlISGBD977gKiHowCo37A+G3/byOb1m6nX0BlkVKxUke2btyfLu2/nPsoFlbvn\n7JGb+5rIgE4hhMg9JMgQaeJb2Jeq1auyYtkKV0BRt35d9u/Zz9HDR12BR6/+vVj/y3pmTJrB0cNH\n+XzJ56z4bAVde3W9Z/lFixfFy9uLjT9v5PzZ81xJuOL2exJCCOFeEmSINIt6OAqHw0G9Bs4go3CR\nwlSsUpGSpUsSXCEYgOo1qzNn4RxWfrmSR+s9yoxJM3ip/0u0eLqFq5yUWjSMRiPD3x7Ol598yRPh\nTzD0hZy5WZoQQohb1O2D8XIrpVQ4sGPHjh2Eh4dnd3U8jqZpxMbH4uXrlelbvWuaxvFTx/Eq6JXh\nrd5tmo2khCTKlSl33/ppmkbSpSSC/IPcsm29EELkNTt37iQiIgIgQtf1nenJKy0ZQgghhHALCTKE\nEEII4RYyhVVkCZs947tVPkheIYQQ2UeCDOFWrp0xEx9sZ0zZ6VIIIXIeCTKEWxmNRvxK+T3w4lmy\n06UQQuQ8EmQItzMajRIgCCFEHiRBRjaz2+3ZvkS2pmlomobjmgNN0zK17KxugbDZZPyGEEJ4Cgky\n0sBdgYDdbifuZBxWmzXTy76XOz/47Q47Z86dQTuTuQEGgMVooXTJ0lkaaMj4DSGE8AwSZNyHOwMB\nzaYRfzoeQz7DXZuAuZPZZMavlF+yD/7AQoGZHkjZbXZsiTbK+5XP0oWxZPyGEEJ4Bgky7sPhcGC1\nWTHmN2Z6IGDQDFiuWvAq6IXRnDUfinabHXuiHaPRmOyD3x1BgKZpJFmTMJvNsvqmEELkQRJkpJHJ\nZHLLB6XJbHK9ssqDTCUVQggh0ko6roUQQgjhFhJkCCGEEMItJMjwYD2f7cnU8VMzvdx33nyHXt17\nZXq5QgghxO1kTEYeNGj4IMqWKpvd1RBCCJHLSZCRydKzpobNZsNms2GwpdygpOs6DocDm2ZjzLAx\nXL18lYnzJma4bg7dgV2zY/Gy4J3PO9MX3rqTpmloNi1d7yPTT4UQIveQICMT2e12Tp45meY1NWya\njVNnT2FONKc4u8RqtXL58mVi3ojh+1Xfo3TFlIlTiH4+GoArCVeYP2s+2zdtx2azEVojlJf6v0SZ\nsmUA+HHtj3z03kcMfH0gn3zwCafiTzFzwUyWzF2C47qD8TPHAzDw+YEEVwrG4mVh9RerMZlNtO3U\nluf7PO+qy/HY40wcPZG/DvxF2XJlGTRiEINeHMS4/46jwSMNUr0/a4IV9LRPkbWYLAT4BUigIYQQ\nuYAEGQ9g3Zp1DOg5gN+P/Y5Siv1799OySUu69e5Gv9f7ATBu+Dg0TWPwm4OZMmoKu7buIuFSAv7l\n/enWuxuhkaFYClgwmU2sX7uez2Z/xqnjp/Dy9sKu7Bw6eIhA/0Ac152tI8tnL+fz9z9n/MLxfLn0\nS+KPxlOhfAUO7jvI3l/3MnTrUGZ+MZMyAWUwe5mxJln5evnXDBo9iIK+BSlcpDAGswFlU3gV9AJA\nGRXfr/ye6Jei+Xjlx+zdsZexQ8cS8XAEdRrUweFwMHLQSPz8/ViwcgFXE64y4+0ZKKUw5zO7yrmT\nUXMGCl6+XmkKMmw2G9arVhwOhwQZQgiRC8jAzwdQt35drl65yv49+wHYsmELhYsUZveW3a61L3Zv\n201kg0gcdgfValVj5qczWf7zcto/2563X32bvw/+jdFs5PKFy0x5dQpPdHqCD9Z+wKTFk/At4UvJ\n0iWZ+PFEGrVoRO1GtWnzUhvKhpalWOlibP1lK/YrdvzK+zFj+QymLpmK1Wrl9e6vo1Aoo8Jus9N/\nZH+q16pO+eDy5C+QH4MyoAzKVUdlUFQMrUivYb0IrBBIm85tqFqzKjs278BkNrF943ZOxp1k7Kyx\nVK5emfB64fR7vR+6rmM0GpOt9XH7y2g2utYXScsrK1c9FUII4X4SZDyAgoUKEhoWysb1GwHYvGEz\nHbt35ODvB7mWeI1/Tv1DXGwctaJqUaJ0Cbr16kbFqhXxC/Cj0/OdiGoUxcZ1zrzn/zmPw+7g4ccf\npqRfSQIrBlKsbDGq1qiKt483Fi8LZouZWnVrcfrUaeKPxWPQDVgsFga+PZDyFctTpWYVAqsFcunc\nJSGm100AAB4KSURBVPZs2QOA2WImqELQfe+lYtWKyY6LlyzO+bPnATh25Bil/EpRpFgR1/Vqtapl\nyjMUQgiRe8lXxwcU9XAUm37bRK9+vdi2eRvdX+nOLz/8wu6tu7l04RIlS5ckIDAAh8PB/Hfns27V\nOv49/S+a1TkgMrJxJADBVYN5qN5D9GrRi4iGEYQ3CMeu3XtlTt2uc/LYSZ566CnXuevXrsP/t3ff\n8VFV+f/HXyeTBAiEjkgaAaSFIiYKQRRk+a4FURErBmXRFV0boijrbxUVRazgKiK4il2s8FX3u9jW\nBhJpoQSQ3gJBUUoIScy08/tjwpiBAAlkMsnk/Xw85kHumXvOfO59hLmfnHvOuRZ2bttJVIMooutE\nl+s4Dh0TYozBem0Fz4aIiMgflGScoN5n9+b9t95nVfYqoqKiSExO5LT001g8fzH5efmk9k4F4PUX\nXue9Ge8xZvwY2nVqR72Yejx1/1P+mRcRERE89tpjrF66mqx5WXzy5ifkbMwhb09ewOf9lP0T8Unx\ntG7bGuuxJLRN4OFpD2OtJX9/PmNuGMONd99I/4H9mffNvEo5xtantOaX3F/Yu3uvvzdj1dJVldK2\niIiEL90uOUG9evciPz+ff73wL3r16QVAanoqSzKXsCRzCWm90wBYsXgF/c7rx/mXnk/7zu2JT4pn\n2+Zth7WXcloKw24fxgsfv4CJMOzK2cVLk16iuLiYXTt38cm7nzB46GDikuJo27ktORtz2LF9B4VF\nhbwx7Q1axLVg4GUDiWkQc1jbv/z8C4P6DyJ/f36FjjG9bzrxSfGMu2Mc639az7KFy3huwnNYaxl7\n51j6n9G/XO1kzsskoVFChT9fRERqJiUZJ6hR40Z07tqZ2e/PJr1POgCn9TqNNdlr2LZpmz/JSGyb\nyILvF7Bi8Qo2r9/MhHsn+Mc8AKxdvpZ3p73L+pXr+XXnr8z7fB5ul5u0Pmk4i51kfp/Jxp82MmDg\nAPr074PH7eGR6Y9Qt35dxt04jtHDRlP8ezEZ12fw0mMvsfuX3WUHbMooMmUUlhIREcGkVydRVFjE\n8AuHM+HeCcS1jQPg/vH3M/uL2eU+X8f6LBERCR+6XVIJ0vukszp7tT/JaNioIW3bt2Xvnr0ktU0C\n4K+j/krutlxuy7iNuvXqMiRjCP3O68dvv/4GQEyDGFYuWsnHr39M4YFCToo7iVvH3cqgjEEAXHvT\ntTw55km++eAb5rwxhyfefIJuPbsx44sZvPLkKyz6bhHrFqxj95bd9Ojdg5gGMZx70bmce9G5gcFa\nGDpyKH3O7OMvmv7B9MOO6ZkZgcuZt27Xmpdnv4zb5SYyKpKR14wE40uoGjdpXGnnUkREwoeSjErw\n8OMP8/DjD+Nyudi203cL5J0v3wnYZ/nS5SxesZjv1nyHMYZ1P61j6OChXDrMN2gzsV0iTTs0pWGb\nhtx7/73M/XYub814i+kzptO0aVMuvvxiJsyY4G9v+BXDueCSC8jdnsvcZXOJTYzluuuuY+BFA/37\nrFm9huefeZ6crTkkt0nmquuuOqwnY8O6DTz31HNkLc6iXr16pJ+Vzt333e1PHEZeO5JT2p9C7rZc\nshZn0bZDW3K35rJ7x26IgEvOvYSLLr2IkbeNZNCfBjHz45l06NQBgIL9BQxIG8B7H73HWf3KXrBL\nRETCl26XVJHTTj+NwoJC1q5eC8CSRUto0rQJq5b/MYBy5bKV9EjtwYa1G3jswcc458/nMP216Qy7\nfhhvvPwGX875MqDNWe/OokOnDkydMZVBgwfx/NPPsyNnB+CbZfLg2Adp3aY1U16ZwrDrh/HylJcD\n6h/Yf4Cbh99Mpy6deGf2O7zwygvs3b2XsXeODdjv3x//G7zQqG4j1i5ai8Pl4KS4k+h/cX++nP8l\nY/4xBtCtEBERCaSejCrSILYBHTp3YPHCxXTq0oklC5dw9XVX8/JLL+P83UleUR65O3Lp2r0rb854\nk9TTUxl6rW/58LjEOLZt2caHMz/kzxf82d9mz949GTTYdzvlqoyrmP3ebJYvXU58Yjxff/k1AKPH\njiYqKoqk1kn8uutXnn/6eX/9d99+l85dOnPLnbf4yx6Y8AAD+w0kZ0sOicmJACQlJ/HsjGcDjufu\nW+8mtmEsTZr6Zpvsz9uPtZryKiIif1BPRhVKPSOVxQsWA7B08VL6D+hPfFI8q7NXk708m2YtmhGX\nEMe2bdtI6ZYSUDelWwq5O3IDLuTJ7ZID9mnSrAn79u4DYNvWbSS3Sw5YzrtTl05QKg9Yv2Y9i35c\nxFmnneV/XXbBZRhj2J6zPbCeiIhIBaknowql9Uzj048+Zd1P64iKiqJ1m9Z06d6F5UuXU1BYQPdT\nu1eovUOX4a7oAlpFhUX0/VNfRt0z6rBeiOYnNff/XK9evWO2ZSJKbpWUasbtcZc7FhERCT/qyahC\nqaencuDAAd5+7W1Sz/At0tW1e1dWLFtB9rJsuqf6koykpCRWZ68OqLtqxSriE+LLPe4hKTmJLRu3\nBDxmfc2qNQEDPzumdGTThk20im9FQlJCwKtu3boVOraDt00OzpYBWPvT2jKnzIqISO2gJKMKxTaM\npX2n9sz5dA6n9zodgC5du7Bx/Ua2b91OStcUXG4Xg68YTNbiLN6c8SZbt2zls39/xiezPmHIVUNw\nuV243C4sFo/H4992uV1Y7x9lfc/pi7WWZyY+w6aNm8icm8mHMz8E63skvdvl5rIrL2Pf3n2MHTWW\nFUtXsGXTFuZ9M48Hxz7oX/bcei1ejxeXyxXw8nq9eL1/lEdERNC1e1denfYqG9ZtYMH8BUz757TD\nzoHGbYiI1B5KMsrJ7XYfdqEt6+V2uXG7jrxvj9QeeDweTk09FY/HQ2z9WOJPiqdxg8Y0qd+Eor1F\ntGreirvuvovv5nzHLdfcwlvT3uLqK64m/Yx0ivYWUbS3CPu7xZXv8m8X7S3C+7sX9wG3/+exfx/L\n5tWbueO6O3h96utck3ENuMBV4MK530ls3VimvjgVd4GbO667g4yBGUweP5mYyBic+U6c+53YYoun\n0INzvzPgZYsOL//73/+O84CT4RcPZ/L4yfz1xr+CJ3DWiWagiIjUHqY2/GVpjEkFlixZsoTU1NQK\n1fV4POTk5uB0O4+5r8vtYvvP24mOjS73Y8sP9ghUpajIKOJaxuFwOIL+WREREeX+HJfLRXFeMW0S\n2gQMWBURkdDJysoiLS0NIM1am1WRuhr4eQwOh4PEuMRyJQIulwss1GlUp1pfJCty4RcRETleSjLK\nweFwlPuiHBUV5X+Vl8fjqdLejGD2niiBERGRg5RkhJjH4yF3Zy5Oz7Fvx9QE0Y5o4lpVza0YERGp\n3pRkhJjX68XpceKIceCIrNkXZo/bg7PQidfrVZIhIiJKMqoLR6SjWo/jKC8PnlCHICIi1YSmsIqI\niEhQqCcjCNzu8i+n7XK5cLvdOFwOTA1fHtPj8vjXEzke5TlvVT1ItrJpYKyI1CZKMipRREQE0ZHR\nOAuc5b5t4HK5cOb7Bn16omr2rQa3y40z30lxTDHeqONLBKIjo4mIKLuDrSJrllRX0ZHRJMYlKtEQ\nkVpBSUYlqsiaGge5XC4wUKdh+dfWuPqSq+nSrQsPPPrA8YZ6TGNuG0P+/nymvzG93HVcLhfF9Ytp\nE3/8i2kd7S99r9eL0+3EUd9R7sXOqhO3242zQANjRaT2qHnf1NVcRdbUOCgqsmJraxhjiIiICOpA\n0YiIiOP6DG+kt8LrhFRUZGRkjR0kq4GxIlKbaOCniIiIBIWSjGquqLCIO0beQYe4DqR1TGP6lMDb\nFwmNEvjiP18ElKUkpfDBOx8AsH3bdhIaJfDp7E8Zcv4Q2rVsx4XnXMimDZtYtmQZA/sNpENcB669\n7Fr27N5z2OdPfnwy3dt2p1NCJ/4++u8VGtQqIiK1m5KMam78P8azMHMhr733Gu/87ztkzs1k5fKV\nFW5n0sRJ3Dn2Tj6f9zmOSAe33XAbjz30GI889QizP5/N5k2beXrC0wF15n47lw3rNvDRnI+YOmMq\ncz6Zw6SJkyrr0EREJMwpyajGCgsKee+t9xg3YRxnnn0mHTt35Nlpzx5Xb8LNo26mb/++nNL+FG74\n2w1kL89m9NjRpPVMo0u3Lgy9bijz584PqBMdHc2kFyfRvmN7/nTunxjzjzHMmD6jsg5PRETCnJKM\namzL5i24XC56pPXwlzVu0ph27dtVuK3OKZ39P7c4qQUAHTt39Jc1P6k5v/36W0CdlG4p1KlTx7+d\n1jONggMF7Ni+o8KfH+7Kum0lIlLbBS3JMMY0Mca8bYzJM8bsNca8bIypX456440xucaYQmPMl8aY\nUw55/1tjjLfUy2OMmRqs46jujDFYawPKyloMKzLqj4lExvgW/So9Q8NgsF57WL1wdryLhomISPkE\nsyfjHaAzMAC4EOgLHHXRBWPMWOA2YCTQEygAPjfGRJfazQIvAS2Bk4FWwL2VHXx1kNwmmcjISJYu\nXuov27d3H5s2bPJvN2vejF9+/sW/vWnDJooKiwLaOZhUVNTq7NUUFxf7t5csXEL9BvWJT4g/rvaC\nreBAAbfdcBvtW7Xn9M6nM2PaDC6/8HIeuu8hANK7pfPsk88y6qZRdEroxNhRYwHI3ZHLzX+5mZSk\nFLq07sL1Q69n+7bt/naXZy1n6CVD6damG50TO3P5wMsDxsWkd0vHGMP1Q68noVECvbv3rtLjFhGp\nroKSZBhjOgHnATdYaxdba+cDtwNXG2NOPkrVUcAj1tp/W2tXAtcBccDgQ/YrtNb+aq3dVfI6EIzj\nCLWY+jFcfe3VPPrAo/zw/Q+sWb2Gu265K2Adjj59+/DaS6+xcsVKlmct57677iM6OjqgnUN7Oo5U\ndiiXy8WYW8ewfu16/vv5f5k0cRIjRo448QMLkofue4gli5bw+vuv887sd8icl8mqFasC9nlpykt0\n6daFL+Z9wZ333onb7Sbj0gwaNmzI/37xv3z81cfUb1CfjCEZ/rEvBw4c4MqMK/n4y4/59OtPaXtK\nW669/FoKCwoB+M+3/8Fay7PTnmXZhmX83zf/V+XHLiJSHQVrMa7ewF5r7dJSZV/h64XoBXx8aAVj\nTBt8PRP/PVhmrd1vjFlQ0t77pXbPMMZcC/wMfIovMQn88z1MPPDoAxQWFjLi6hE0aNCAkbePJD8/\n3//+uMfGcfctd3PZBZfR8uSWjH9iPCuXBc4+Kasnozy9G2f1O4s27dow5PwhuFwuBl8xmLvuu+vE\nDyoICg4U8OHMD5n66lTOPPtMACZNnURqx9SA/fr068PI20b6t2e9NwtrLU8+96S/7JkXniElKYX5\nc+fTt39f+vTtE9DG488+ziezPiFzXiYDzhtA02ZNAWjYqCHNWzQP1iGKiNQ4wUoyTgZ2lS6w1nqM\nMXtK3jtSHQv8ckj5L4fUeRvYCuQC3YEngQ7A5ScedvUTUz+Gf07/J/+c/k9/2c233+z/ueXJLXlr\n1lsBdVZt++Ov94SkBHL25QS83/us3oeVXZlxJVdmXOnfnvziZP/P1TWxKG3rlq243W5OTT3VXxbb\nMPawQbLde3QP2F69cjWbN26mQ1yHgHJnsZOtm7dCf/jt1994YvwTZM7LZPdvu/F4PPxe9LsGwIqI\nHEOFkgxjzERg7FF2sfjGYQSNtfblUpurjDE7gf8aY9pYazcfre7o0aNp1KhRQNnQoUMZOnRoECKV\n6igmJiZgu+BAAd1P684Lr7xw2C2kZs2bATBq5Cjy9uXx6FOPEp8YT3R0NBcNuAiXUwNHRSS8zJw5\nk5kzZwaU5eXlHXd7Fe3JeBp49Rj7bMJ3G+Ok0oXGGAfQtOS9svwMGHwDOkv3ZrQElpZZw2dhSb1T\ngKMmGZMnTyY1NfVou0gN1Tq5NZGRkSzPWk5cfBwA+/P2s2nDJtL7pB+xXrce3fh09qc0a96M+g3K\nnvy0eOFiJk6ayDn/cw4AO7bvOGx11KioKDwePZdERGq2sv7wzsrKIi0t7bjaq9DAT2vtbmvtumO8\n3EAm0NgYc1qp6gPwJQMLjtD2ZnyJxoCDZcaYhvjGcMwvq06J0/D1oOysyLFIeKnfoD5XXHMFj/zj\nEebPnc/an9Yy5vYxOByOo44/ufTKS2narCkjho5gYeZCcrbmMH/ufMbdO46fd/ry4Tbt2vDRux+x\nYd0GshZlcceNd1Avpl5AOwlJCcz7dh6/7vqVvH3Hn/WLiISToMwusdauAT4H/mWMOcMY0wd4Hphp\nrfX3ZBhj1hhjLilV9VngfmPMRcaYbsAbwHZKBooaY9oaY+43xqQaY1obYy4GXge+K5mNIrXYQxMf\nIq1XGn+56i9cM/gaeqb3pF37dtSp61tQrKxko169esz6bBbxCfHcOOxGzul5Dvfcfg/FzmJiY2MB\n30DQvH15nN/3fO68+U5u+NsNhw3wHDdhHN9/8z09U3pyft/zg3+wIiI1gCnPVMbjatiYxsAU4CLA\nC3wIjLLWFpbaxwOMsNa+UarsIXzrZDQG5gK3Wms3lLyXALwFdAHqAznALGDC0aaxGmNSgSVLliyp\ndrdLXC4Xm7dvpk6jOjX28eUHuVwuivOKaZPQJijHUtFzVVRYRFqnNB587EGuGnZVpcdTUcE+PyIi\nwVDqdkmatTarInWDNbsEa+0+YNgx9nGUUfYQ8NAR9t8OnHPi0Uk4WrliJRvXbaRHWg/25+1n8hOT\nMRjOHXhuqEMTEamVgpZkSMWEwyPUq8MxTHt+Gps2bCIqKoruPboz+4vZNGnaJNRhiYjUSkoyQiwi\nIoLoyGicBU481PzZCdGR0UREhOa5e127d2XOd3NC8tkiInI4JRkh5nA4SIxLxOv1hjqUShERERGw\n7LmIiNReSjKqAYfDoQuziIiEndD0a4uIiEjYU0+G1DjVYYDp8aipcYuIHC8lGVJjhMMg2VAOjBUJ\nVx6Pp9LGtWlcWeVSkiE1RjgMktUXmEjl8ng85OTm4HQ7K6W96MhoEuMS9f+0kijJkBpFg2RFpDSv\n14vT7cRR30Fk5Ild0txuN84CJ16vV98zlURJhoiI1HiRkZGVslx/Tb0VW13p5rCIiIgEhZIMERER\nCQolGSIiIhIUSjJEREQkKJRkiIiISFAoyRAREZGgUJIhIiJh69uvvuXS8y4lJSmFrsldGX7lcLZu\n3hrqsGoNJRkiIhK2CgsLuen2m/js+894/9/v43A4uCHjhlCHVWtoMS4REQlbAy8eGLD99JSn6d62\nO+vWrKNDpw4hiqr2UJIhIiJha/PGzTw94WmWLl7Knj178Hq9GGPYkbNDSUYVUJIhIiJha/iVw0lq\nncRTU56iZauWWK+lf8/+uFyuUIdWKyjJEBGRsLR3z142bdjEMy88wxnpZwCwMHNhiKOqXZRkiIhI\nWGrcpDFNmjbhrVffosVJLdies53HH3ocY0yoQ6s1NLtERETCkjGGF197kexl2QzoPYDx/xjPAxMe\nCHVYtYp6MkREJGyd1e8svl7wdUBZzr6cEEVT+6gnQ0RERIJCPRkiIiLi5/F48Hq9/u1SM3EijTFR\nx6jutdZ6/BUqPzwRERGpiTweDzm5OTjdTn9Z7m+5EAXE0ooW7D1qA/txGmO2H0w0lGSIiIgIAF6v\nF6fbiaO+g8hIX4oQHRsNdYCTcdKf4iNWzieSBUSzkwhASYaIiIgcLjIykqioKP/PGCASDy1wH6Oq\no/SGBn6KiIhIUKgnQ0REajy3+1h/YFdNGxJISYaIiNRYERERREdG4yxw4sFz7ArHEB0ZTUSEOvkr\ni5IMERGpsRwOB4lxiQFTLk9EREQEDofj2DtKuSjJkLB26Hzv6kRfZiKVw+Fw6P9SNaUkQ8JWWfO9\nq5PoyGgS4xL15SgiYUtJhoStsuZ7VxdutxtngROv16skQ0TCVvX65hUJgtLzvauTyhikJiJSnWkI\nrYiIiASFkgwREREJCiUZIiIiEhRKMkQqQXq3dF558RX/dkKjBL74zxchjEhEJPSUZIiIiEj5TOYv\nPMq75d1dSYaIiIiUj5umeGhd3t2VZEit9NVnX5GSlIK1FoBV2atIaJTAxIcn+vcZc9sY7hh5BwAL\nMxcy5PwhtGvZjp5dejLu3nEUFRaFJHYRkZC5h0k8SO/y7q4kIwzMnDkz1CHUOL3O7EXBgQJWLl8J\nwI/zfqRZ82Zkzs307/PjDz/Sp28ftm7eyrDLhjFo8CC+/vFrho4YyqIFi7j/nvtDFX6to9/xqqXz\nXbXC+XwryQgD4fwLGiyxDWNJ6ZbC/HnzAcicl8mNt97IqhWrKCosYmfuTrZu3kqvM3sxZdIUhlw1\nhOtvvp7WbVqzfOVyHn78YT545wOczuq5ZHm40e941dL5rlrhfL6VZEitld4n3d9zsWD+Ai646AJO\n6XgKCzMXsuCHBbRs1ZLktsmszl7NB29/QIe4DnSI68DXn3xNxpAMALZt3RbKQxARqdaUZEit1fvs\n3izKXMSq7FVER0fTrn070vukM3/ufDJ/yKR3H99tx4KCAjJGZPDV/K/48ocvSR+Qzlfzv2Ju1lyS\n2ySH9BhERKozPbtEaq1evXuRn5/Pv174F+l90gE48+wzmTJpCvvz9nPT7TcB0O3Ubqxfs56k5CQA\nYurH0LpNuQdXi4jUWrUlyagL8NNPP4U6jqDIy8sjKysr1GFUOy6Xi9zfcomOjT7iU1iT2yUz671Z\n3Hr3rWRnZxPbOJbsZdl4PB6atmhKdnY2fx70Z0bfNJq/jfgb5190Prt+3sVLU19i6aKl3HLXLQA4\nnU5yd+aSnZ3tb3vLli0B26W53W6c+U727tpbLR/eVt3od7xq6XxXrep0vsv63ly/fr3vTbfvWloR\n5uAUvnBmjLkGeDvUcUgIRAF1AHOE938HXEB9/rh5WABYoEGp/TxAccm/lOwbWdI2wAEguuQFkA/U\n48hpvC1pz1X+QxERqRIHvzcP1YTbuZhPjlgvn0gW4GAtW6y1Lqg9SUYz4DxgC77LitQOkTSlFSfj\nJLKaPVfdjYOfiWYPOwF3qMMRESkRQQwnUZc/uli91MVNPI2YSwP2HbX2fpzsZLu11gO1JMmQ2skY\nE0VHkvkfimlRzS7kvxLJV9QpnfGLiFQHxhgHxz8xxHswwYDaMyZDREREyqEkSaiU3l9NYRUREZGg\nUJIhIiIiQaEkQ0RERIJCSYaIiIgEhQZ+hhFjzP8DLgR6AMXW2qYhDql6yK/E3/NZXMcBRmJpgYPV\nxPMg/VkR0pjClDHmbOAeIA1oBQy21h55jr6cEGPMfcClQCegCJgPjLXWrgtpYGHKGHMz8DcguaRo\nFTDeWvtZyIIKAn3RhZco4H0gE7g+xLFUB17242QB0YDjhFvLZSC/cz91eIC6LOcAI1jPm+zjXGKP\nMXe8LPtxAt4Tjit81QeWAa8As0IcS21wNvA8sBjftWEi8IUxprO1tiikkYWnHGAssB7fcoF/AT42\nxvSw1obN8tRaJyMMGWOGA5PVk3HC870PNQ9YCNxVqmwzMAV45jjaC5hPLkdmjPGinowqZYxpDuwC\n+lpr54U6ntrAGLMbGGOtfTXUsVQW9WRIWKus+d7GmCggFZhQevEsY8xXQC8tqCVhqDG+BfD3hDqQ\ncGeMiQCuBGLw9USHDSUZIuXTHN8tl18OKf8F6Fj14YgEjzHGAM8C86y1q0MdT7gyxnTFl1TUxffE\no0uttWtCG1Xl0uySas4YM9EY4z3Ky2OM6RDqOEUkrEwFUoCrQx1ImFsDnAr0BF4E3jDGdAptSJVL\nPRnV39PAse7PbaqKQGq53/Dddml5SHlL4OeqD0ckOIwxU4CBwNnW2p2hjiecWWvd/PH9vdQY0xMY\nhW/WSVhQklHNWWt3A7tDHUdtZ611GWOWAAPA96jjki7lAcBzoYxNpLKUJBiXAP2stdtCHU8tFEHZ\nD1mvsZRkhBFjTCLQFGgNOIwxp5a8tcFaWxC6yMLGJOC1kmRjITAa30Ct10IZVLgyxtQHTsE3vQ+g\nbcnv9B5rbU7oIgtPxpipwFDgYqDAGHOw1y7PWvt76CILT8aYx4A5wDYgFsgA+gHnhjKuyqYprGHE\nGPMqcF0Zb/W31n5f1fGEI2PMLcC9+G6TLANut9YuDm1U4ckY0w/4Bt8Mh9Jet9ZqHZhKVjJNuKwL\nwghr7RtVHU+4M8a8DPwJ30JzecAK4HFr7dchDaySKckQERGRoNDsEhEREQkKJRkiIiISFEoyRERE\nJCiUZIiIiEhQKMkQERGRoFCSISIiIkGhJENERESCQkmGiIiIBIWSDBEREQkKJRkiIiISFEoyRERE\nJCj+PwxAfrJxjHI2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x220d73d6a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs224d.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from q3_word2vec import *\n",
    "from q3_sgd import *\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / \\\n",
    "\tdimVectors, np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, \n",
    "    \tnegSamplingCostAndGradient), \n",
    "    wordVectors, 0.3, 10000, None, True, PRINT_EVERY=10)\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "# Visualize the word vectors you trained\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \n",
    "\t\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "\t\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \n",
    "\t\"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], \n",
    "    \tbbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for softmax regression ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(1.7197198035178798, array([[ 1.04574436, -0.22392156, -0.2089484 , -0.05144186, -0.13019251],\n",
      "       [ 0.13035736, -0.13642677, -0.06661226,  0.03975088, -0.02914031],\n",
      "       [ 0.31652344, -0.15724945, -0.0733285 , -0.06836636, -0.13648043],\n",
      "       [-0.35224321,  0.3329344 ,  0.12169607,  0.03919428,  0.08952855],\n",
      "       [-1.04637139,  0.36240193,  0.31686771,  0.25696214,  0.04312799],\n",
      "       [-0.10855243,  0.02499814,  0.30659762,  0.15581989,  0.09873214],\n",
      "       [ 0.03505001, -0.21385379, -0.28610887, -0.18439528,  0.04576804],\n",
      "       [-0.26395267,  0.14052696,  0.08730093,  0.18158638, -0.01655735],\n",
      "       [ 0.66275649, -0.0438666 , -0.21191675, -0.10284391, -0.19610518],\n",
      "       [ 0.27787294, -0.08915023,  0.01094335, -0.13142639,  0.02776031]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64))\n",
      "2.30258509299\n",
      "2.46018404561\n",
      "4.38202663467\n",
      "4.57849668347\n",
      "6.90775527898\n",
      "7.10925581034\n",
      "0.000817503143173\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cs224d.data_utils import *\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q3_sgd import load_saved_params\n",
    "\n",
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    \"\"\" Obtain the sentence feature for sentiment analysis by averaging its word vectors \"\"\"\n",
    "    # Implement computation for the sentence features given a sentence.                                                       \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #          the word vector list                                \n",
    "    # - wordVectors: word vectors (each row) for all tokens                \n",
    "    # - sentence: a list of words in the sentence of interest \n",
    "\n",
    "    # Output:                                                         \n",
    "    # - sentVector: feature vector for the sentence    \n",
    "    \n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    array = np.fromiter( (tokens[word] for word in sentence), dtype='int')\n",
    "    sentVector = np.mean(wordVectors[array], axis=0)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return sentVector\n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    \"\"\" Softmax Regression \"\"\"\n",
    "    # Implement softmax regression with weight regularization.\n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - features: feature vectors, each row is a feature vector\n",
    "    # - labels: labels corresponding to the feature vectors\n",
    "    # - weights: weights of the regressor\n",
    "    # - regularization: L2 regularization constant\n",
    "    \n",
    "    # Output:\n",
    "    # - cost: cost of the regressor\n",
    "    # - grad: gradient of the regressor cost with respect to its\n",
    "    #        weights\n",
    "    # - pred: label predictions of the regressor (you might find\n",
    "    #        np.argmax helpful)\n",
    "    \n",
    "    prob = softmax(features.dot(weights))\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "\n",
    "    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n",
    "    cost = np.sum(-np.log(prob[np.arange(N), labels] + 1e-12)) / N\n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2)\n",
    "    \n",
    "    ### YOUR CODE HERE: compute the gradients and predictions\n",
    "    pred = np.argmax(prob, axis=1)\n",
    "    dx = prob\n",
    "    dx[np.arange(N), labels] -= 1\n",
    "    dx /= N\n",
    "    # dx is the gradient associated with the loss (softmax layer only)\n",
    "    grad = np.dot(features.T, dx)\n",
    "    #backprop the weights\n",
    "    grad += regularization * weights\n",
    "    #adding the regularization to the gradient\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, \n",
    "        regularization)\n",
    "    return cost, grad\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Run python q4_softmaxreg.py.\n",
    "    \"\"\"\n",
    "    random.seed(314159)\n",
    "    np.random.seed(265)\n",
    "\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "\n",
    "    _, wordVectors0, _ = load_saved_params()\n",
    "    N = wordVectors0.shape[0]//2\n",
    "    #assert N == nWords\n",
    "    wordVectors = (wordVectors0[:N,:] + wordVectors0[N:,:])\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "\n",
    "    dummy_weights = 0.1 * np.random.randn(dimVectors, 5)\n",
    "    dummy_features = np.zeros((10, dimVectors))\n",
    "    dummy_labels = np.zeros((10,), dtype=np.int32)    \n",
    "    for i in range(10):\n",
    "        words, dummy_labels[i] = dataset.getRandomTrainSentence()\n",
    "        dummy_features[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    print(\"==== Gradient check for softmax regression ====\")\n",
    "    gradcheck_naive(lambda weights: softmaxRegression(dummy_features,\n",
    "        dummy_labels, weights, 1.0, nopredictions = True), dummy_weights)\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(softmaxRegression(dummy_features, dummy_labels, dummy_weights, 1.0))\n",
    "\n",
    "    dummy_weights  = 0.1 * np.random.randn(40, 10) + 1.0\n",
    "    dummy_features = np.random.randn(2000, 40)\n",
    "    dummy_labels   = np.argmax(np.random.randn(2000, 10), axis=1)\n",
    "\n",
    "    print(-np.log(0.1))#expected correct classification (random) = 1 in 10;\n",
    "    #cost then becomes -np.log(0.1)\n",
    "    print(softmaxRegression(dummy_features, dummy_labels, dummy_weights, 0.0)[0])\n",
    "\n",
    "    dummy_weights  = 0.1 * np.random.randn(40, 80) + 1.0\n",
    "    dummy_features = np.random.randn(2000, 40)\n",
    "    dummy_labels   = np.argmax(np.random.randn(2000, 80), axis=1)\n",
    "\n",
    "    print(-np.log(1./80))#expected correct classification (random) = 1 in 80;\n",
    "    #cost then becomes -np.log(1./80)\n",
    "    print(softmaxRegression(dummy_features, dummy_labels, dummy_weights, 0.0)[0])\n",
    "\n",
    "    dummy_weights  = 0.1 * np.random.randn(40, 1000) + 1.0\n",
    "    dummy_features = np.random.randn(40000, 40)\n",
    "    dummy_labels   = np.argmax(np.random.randn(40000, 1000), axis=1)\n",
    "\n",
    "    print(-np.log(1./1000))#expected correct classification (random) = 1 in 80;\n",
    "    #cost then becomes -np.log(1./80)\n",
    "    print(softmaxRegression(dummy_features, dummy_labels, dummy_weights, 0.0)[0])\n",
    "    print(np.exp(-softmaxRegression(dummy_features, dummy_labels, dummy_weights, 0.0)[0]))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for reg=0.000000\n",
      "iter 100: 0.001622\n",
      "iter 200: 0.001621\n",
      "iter 300: 0.001618\n",
      "iter 400: 0.001615\n",
      "iter 500: 0.001611\n",
      "iter 600: 0.001606\n",
      "iter 700: 0.001601\n",
      "iter 800: 0.001595\n",
      "iter 900: 0.001589\n",
      "iter 1000: 0.001583\n",
      "iter 1100: 0.001577\n",
      "iter 1200: 0.001570\n",
      "iter 1300: 0.001564\n",
      "iter 1400: 0.001557\n",
      "iter 1500: 0.001551\n",
      "iter 1600: 0.001544\n",
      "iter 1700: 0.001537\n",
      "iter 1800: 0.001531\n",
      "iter 1900: 0.001524\n",
      "iter 2000: 0.001518\n",
      "iter 2100: 0.001511\n",
      "iter 2200: 0.001505\n",
      "iter 2300: 0.001498\n",
      "iter 2400: 0.001492\n",
      "iter 2500: 0.001486\n",
      "iter 2600: 0.001480\n",
      "iter 2700: 0.001474\n",
      "iter 2800: 0.001469\n",
      "iter 2900: 0.001463\n",
      "iter 3000: 0.001457\n",
      "iter 3100: 0.001452\n",
      "iter 3200: 0.001446\n",
      "iter 3300: 0.001441\n",
      "iter 3400: 0.001436\n",
      "iter 3500: 0.001431\n",
      "iter 3600: 0.001426\n",
      "iter 3700: 0.001421\n",
      "iter 3800: 0.001417\n",
      "iter 3900: 0.001412\n",
      "iter 4000: 0.001408\n",
      "iter 4100: 0.001403\n",
      "iter 4200: 0.001399\n",
      "iter 4300: 0.001395\n",
      "iter 4400: 0.001391\n",
      "iter 4500: 0.001387\n",
      "iter 4600: 0.001383\n",
      "iter 4700: 0.001379\n",
      "iter 4800: 0.001376\n",
      "iter 4900: 0.001372\n",
      "iter 5000: 0.001368\n",
      "iter 5100: 0.001365\n",
      "iter 5200: 0.001362\n",
      "iter 5300: 0.001358\n",
      "iter 5400: 0.001355\n",
      "iter 5500: 0.001352\n",
      "iter 5600: 0.001349\n",
      "iter 5700: 0.001346\n",
      "iter 5800: 0.001343\n",
      "iter 5900: 0.001340\n",
      "iter 6000: 0.001337\n",
      "iter 6100: 0.001335\n",
      "iter 6200: 0.001332\n",
      "iter 6300: 0.001329\n",
      "iter 6400: 0.001327\n",
      "iter 6500: 0.001324\n",
      "iter 6600: 0.001322\n",
      "iter 6700: 0.001320\n",
      "iter 6800: 0.001317\n",
      "iter 6900: 0.001315\n",
      "iter 7000: 0.001313\n",
      "iter 7100: 0.001311\n",
      "iter 7200: 0.001309\n",
      "iter 7300: 0.001307\n",
      "iter 7400: 0.001305\n",
      "iter 7500: 0.001303\n",
      "iter 7600: 0.001301\n",
      "iter 7700: 0.001299\n",
      "iter 7800: 0.001297\n",
      "iter 7900: 0.001295\n",
      "iter 8000: 0.001293\n",
      "iter 8100: 0.001292\n",
      "iter 8200: 0.001290\n",
      "iter 8300: 0.001288\n",
      "iter 8400: 0.001287\n",
      "iter 8500: 0.001285\n",
      "iter 8600: 0.001283\n",
      "iter 8700: 0.001282\n",
      "iter 8800: 0.001280\n",
      "iter 8900: 0.001279\n",
      "iter 9000: 0.001277\n",
      "iter 9100: 0.001276\n",
      "iter 9200: 0.001275\n",
      "iter 9300: 0.001273\n",
      "iter 9400: 0.001272\n",
      "iter 9500: 0.001271\n",
      "iter 9600: 0.001269\n",
      "iter 9700: 0.001268\n",
      "iter 9800: 0.001267\n",
      "iter 9900: 0.001266\n",
      "iter 10000: 0.001264\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000001\n",
      "iter 100: 0.001664\n",
      "iter 200: 0.001663\n",
      "iter 300: 0.001661\n",
      "iter 400: 0.001657\n",
      "iter 500: 0.001653\n",
      "iter 600: 0.001649\n",
      "iter 700: 0.001644\n",
      "iter 800: 0.001638\n",
      "iter 900: 0.001633\n",
      "iter 1000: 0.001627\n",
      "iter 1100: 0.001621\n",
      "iter 1200: 0.001615\n",
      "iter 1300: 0.001608\n",
      "iter 1400: 0.001602\n",
      "iter 1500: 0.001596\n",
      "iter 1600: 0.001589\n",
      "iter 1700: 0.001583\n",
      "iter 1800: 0.001577\n",
      "iter 1900: 0.001570\n",
      "iter 2000: 0.001564\n",
      "iter 2100: 0.001558\n",
      "iter 2200: 0.001552\n",
      "iter 2300: 0.001546\n",
      "iter 2400: 0.001540\n",
      "iter 2500: 0.001534\n",
      "iter 2600: 0.001529\n",
      "iter 2700: 0.001523\n",
      "iter 2800: 0.001518\n",
      "iter 2900: 0.001512\n",
      "iter 3000: 0.001507\n",
      "iter 3100: 0.001502\n",
      "iter 3200: 0.001497\n",
      "iter 3300: 0.001492\n",
      "iter 3400: 0.001487\n",
      "iter 3500: 0.001482\n",
      "iter 3600: 0.001478\n",
      "iter 3700: 0.001473\n",
      "iter 3800: 0.001469\n",
      "iter 3900: 0.001465\n",
      "iter 4000: 0.001460\n",
      "iter 4100: 0.001456\n",
      "iter 4200: 0.001452\n",
      "iter 4300: 0.001448\n",
      "iter 4400: 0.001445\n",
      "iter 4500: 0.001441\n",
      "iter 4600: 0.001437\n",
      "iter 4700: 0.001434\n",
      "iter 4800: 0.001430\n",
      "iter 4900: 0.001427\n",
      "iter 5000: 0.001424\n",
      "iter 5100: 0.001421\n",
      "iter 5200: 0.001417\n",
      "iter 5300: 0.001414\n",
      "iter 5400: 0.001411\n",
      "iter 5500: 0.001409\n",
      "iter 5600: 0.001406\n",
      "iter 5700: 0.001403\n",
      "iter 5800: 0.001400\n",
      "iter 5900: 0.001398\n",
      "iter 6000: 0.001395\n",
      "iter 6100: 0.001393\n",
      "iter 6200: 0.001390\n",
      "iter 6300: 0.001388\n",
      "iter 6400: 0.001386\n",
      "iter 6500: 0.001383\n",
      "iter 6600: 0.001381\n",
      "iter 6700: 0.001379\n",
      "iter 6800: 0.001377\n",
      "iter 6900: 0.001375\n",
      "iter 7000: 0.001373\n",
      "iter 7100: 0.001371\n",
      "iter 7200: 0.001369\n",
      "iter 7300: 0.001367\n",
      "iter 7400: 0.001365\n",
      "iter 7500: 0.001364\n",
      "iter 7600: 0.001362\n",
      "iter 7700: 0.001360\n",
      "iter 7800: 0.001359\n",
      "iter 7900: 0.001357\n",
      "iter 8000: 0.001355\n",
      "iter 8100: 0.001354\n",
      "iter 8200: 0.001352\n",
      "iter 8300: 0.001351\n",
      "iter 8400: 0.001349\n",
      "iter 8500: 0.001348\n",
      "iter 8600: 0.001347\n",
      "iter 8700: 0.001345\n",
      "iter 8800: 0.001344\n",
      "iter 8900: 0.001343\n",
      "iter 9000: 0.001341\n",
      "iter 9100: 0.001340\n",
      "iter 9200: 0.001339\n",
      "iter 9300: 0.001338\n",
      "iter 9400: 0.001337\n",
      "iter 9500: 0.001335\n",
      "iter 9600: 0.001334\n",
      "iter 9700: 0.001333\n",
      "iter 9800: 0.001332\n",
      "iter 9900: 0.001331\n",
      "iter 10000: 0.001330\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000002\n",
      "iter 100: 0.001708\n",
      "iter 200: 0.001706\n",
      "iter 300: 0.001704\n",
      "iter 400: 0.001701\n",
      "iter 500: 0.001697\n",
      "iter 600: 0.001693\n",
      "iter 700: 0.001688\n",
      "iter 800: 0.001683\n",
      "iter 900: 0.001677\n",
      "iter 1000: 0.001671\n",
      "iter 1100: 0.001666\n",
      "iter 1200: 0.001660\n",
      "iter 1300: 0.001654\n",
      "iter 1400: 0.001648\n",
      "iter 1500: 0.001641\n",
      "iter 1600: 0.001635\n",
      "iter 1700: 0.001629\n",
      "iter 1800: 0.001623\n",
      "iter 1900: 0.001617\n",
      "iter 2000: 0.001611\n",
      "iter 2100: 0.001605\n",
      "iter 2200: 0.001600\n",
      "iter 2300: 0.001594\n",
      "iter 2400: 0.001588\n",
      "iter 2500: 0.001583\n",
      "iter 2600: 0.001577\n",
      "iter 2700: 0.001572\n",
      "iter 2800: 0.001567\n",
      "iter 2900: 0.001562\n",
      "iter 3000: 0.001557\n",
      "iter 3100: 0.001552\n",
      "iter 3200: 0.001547\n",
      "iter 3300: 0.001542\n",
      "iter 3400: 0.001538\n",
      "iter 3500: 0.001533\n",
      "iter 3600: 0.001529\n",
      "iter 3700: 0.001525\n",
      "iter 3800: 0.001521\n",
      "iter 3900: 0.001517\n",
      "iter 4000: 0.001513\n",
      "iter 4100: 0.001509\n",
      "iter 4200: 0.001505\n",
      "iter 4300: 0.001502\n",
      "iter 4400: 0.001498\n",
      "iter 4500: 0.001495\n",
      "iter 4600: 0.001491\n",
      "iter 4700: 0.001488\n",
      "iter 4800: 0.001485\n",
      "iter 4900: 0.001482\n",
      "iter 5000: 0.001478\n",
      "iter 5100: 0.001476\n",
      "iter 5200: 0.001473\n",
      "iter 5300: 0.001470\n",
      "iter 5400: 0.001467\n",
      "iter 5500: 0.001464\n",
      "iter 5600: 0.001462\n",
      "iter 5700: 0.001459\n",
      "iter 5800: 0.001457\n",
      "iter 5900: 0.001454\n",
      "iter 6000: 0.001452\n",
      "iter 6100: 0.001450\n",
      "iter 6200: 0.001448\n",
      "iter 6300: 0.001445\n",
      "iter 6400: 0.001443\n",
      "iter 6500: 0.001441\n",
      "iter 6600: 0.001439\n",
      "iter 6700: 0.001437\n",
      "iter 6800: 0.001435\n",
      "iter 6900: 0.001433\n",
      "iter 7000: 0.001432\n",
      "iter 7100: 0.001430\n",
      "iter 7200: 0.001428\n",
      "iter 7300: 0.001426\n",
      "iter 7400: 0.001425\n",
      "iter 7500: 0.001423\n",
      "iter 7600: 0.001422\n",
      "iter 7700: 0.001420\n",
      "iter 7800: 0.001419\n",
      "iter 7900: 0.001417\n",
      "iter 8000: 0.001416\n",
      "iter 8100: 0.001414\n",
      "iter 8200: 0.001413\n",
      "iter 8300: 0.001412\n",
      "iter 8400: 0.001410\n",
      "iter 8500: 0.001409\n",
      "iter 8600: 0.001408\n",
      "iter 8700: 0.001407\n",
      "iter 8800: 0.001405\n",
      "iter 8900: 0.001404\n",
      "iter 9000: 0.001403\n",
      "iter 9100: 0.001402\n",
      "iter 9200: 0.001401\n",
      "iter 9300: 0.001400\n",
      "iter 9400: 0.001399\n",
      "iter 9500: 0.001398\n",
      "iter 9600: 0.001397\n",
      "iter 9700: 0.001396\n",
      "iter 9800: 0.001395\n",
      "iter 9900: 0.001394\n",
      "iter 10000: 0.001393\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000004\n",
      "iter 100: 0.001795\n",
      "iter 200: 0.001793\n",
      "iter 300: 0.001791\n",
      "iter 400: 0.001788\n",
      "iter 500: 0.001785\n",
      "iter 600: 0.001781\n",
      "iter 700: 0.001776\n",
      "iter 800: 0.001771\n",
      "iter 900: 0.001766\n",
      "iter 1000: 0.001761\n",
      "iter 1100: 0.001755\n",
      "iter 1200: 0.001750\n",
      "iter 1300: 0.001744\n",
      "iter 1400: 0.001739\n",
      "iter 1500: 0.001733\n",
      "iter 1600: 0.001727\n",
      "iter 1700: 0.001722\n",
      "iter 1800: 0.001716\n",
      "iter 1900: 0.001711\n",
      "iter 2000: 0.001705\n",
      "iter 2100: 0.001700\n",
      "iter 2200: 0.001694\n",
      "iter 2300: 0.001689\n",
      "iter 2400: 0.001684\n",
      "iter 2500: 0.001679\n",
      "iter 2600: 0.001674\n",
      "iter 2700: 0.001669\n",
      "iter 2800: 0.001664\n",
      "iter 2900: 0.001660\n",
      "iter 3000: 0.001655\n",
      "iter 3100: 0.001651\n",
      "iter 3200: 0.001646\n",
      "iter 3300: 0.001642\n",
      "iter 3400: 0.001638\n",
      "iter 3500: 0.001634\n",
      "iter 3600: 0.001630\n",
      "iter 3700: 0.001626\n",
      "iter 3800: 0.001622\n",
      "iter 3900: 0.001619\n",
      "iter 4000: 0.001615\n",
      "iter 4100: 0.001612\n",
      "iter 4200: 0.001608\n",
      "iter 4300: 0.001605\n",
      "iter 4400: 0.001602\n",
      "iter 4500: 0.001599\n",
      "iter 4600: 0.001596\n",
      "iter 4700: 0.001593\n",
      "iter 4800: 0.001590\n",
      "iter 4900: 0.001587\n",
      "iter 5000: 0.001584\n",
      "iter 5100: 0.001582\n",
      "iter 5200: 0.001579\n",
      "iter 5300: 0.001577\n",
      "iter 5400: 0.001574\n",
      "iter 5500: 0.001572\n",
      "iter 5600: 0.001569\n",
      "iter 5700: 0.001567\n",
      "iter 5800: 0.001565\n",
      "iter 5900: 0.001563\n",
      "iter 6000: 0.001561\n",
      "iter 6100: 0.001559\n",
      "iter 6200: 0.001557\n",
      "iter 6300: 0.001555\n",
      "iter 6400: 0.001553\n",
      "iter 6500: 0.001551\n",
      "iter 6600: 0.001549\n",
      "iter 6700: 0.001548\n",
      "iter 6800: 0.001546\n",
      "iter 6900: 0.001544\n",
      "iter 7000: 0.001543\n",
      "iter 7100: 0.001541\n",
      "iter 7200: 0.001540\n",
      "iter 7300: 0.001538\n",
      "iter 7400: 0.001537\n",
      "iter 7500: 0.001535\n",
      "iter 7600: 0.001534\n",
      "iter 7700: 0.001532\n",
      "iter 7800: 0.001531\n",
      "iter 7900: 0.001530\n",
      "iter 8000: 0.001529\n",
      "iter 8100: 0.001527\n",
      "iter 8200: 0.001526\n",
      "iter 8300: 0.001525\n",
      "iter 8400: 0.001524\n",
      "iter 8500: 0.001523\n",
      "iter 8600: 0.001522\n",
      "iter 8700: 0.001521\n",
      "iter 8800: 0.001520\n",
      "iter 8900: 0.001518\n",
      "iter 9000: 0.001517\n",
      "iter 9100: 0.001517\n",
      "iter 9200: 0.001516\n",
      "iter 9300: 0.001515\n",
      "iter 9400: 0.001514\n",
      "iter 9500: 0.001513\n",
      "iter 9600: 0.001512\n",
      "iter 9700: 0.001511\n",
      "iter 9800: 0.001510\n",
      "iter 9900: 0.001509\n",
      "iter 10000: 0.001509\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000008\n",
      "iter 100: 0.001970\n",
      "iter 200: 0.001969\n",
      "iter 300: 0.001967\n",
      "iter 400: 0.001964\n",
      "iter 500: 0.001961\n",
      "iter 600: 0.001957\n",
      "iter 700: 0.001953\n",
      "iter 800: 0.001949\n",
      "iter 900: 0.001944\n",
      "iter 1000: 0.001940\n",
      "iter 1100: 0.001935\n",
      "iter 1200: 0.001930\n",
      "iter 1300: 0.001925\n",
      "iter 1400: 0.001920\n",
      "iter 1500: 0.001915\n",
      "iter 1600: 0.001910\n",
      "iter 1700: 0.001905\n",
      "iter 1800: 0.001900\n",
      "iter 1900: 0.001895\n",
      "iter 2000: 0.001890\n",
      "iter 2100: 0.001885\n",
      "iter 2200: 0.001880\n",
      "iter 2300: 0.001876\n",
      "iter 2400: 0.001871\n",
      "iter 2500: 0.001867\n",
      "iter 2600: 0.001862\n",
      "iter 2700: 0.001858\n",
      "iter 2800: 0.001854\n",
      "iter 2900: 0.001849\n",
      "iter 3000: 0.001845\n",
      "iter 3100: 0.001841\n",
      "iter 3200: 0.001837\n",
      "iter 3300: 0.001834\n",
      "iter 3400: 0.001830\n",
      "iter 3500: 0.001826\n",
      "iter 3600: 0.001823\n",
      "iter 3700: 0.001819\n",
      "iter 3800: 0.001816\n",
      "iter 3900: 0.001812\n",
      "iter 4000: 0.001809\n",
      "iter 4100: 0.001806\n",
      "iter 4200: 0.001803\n",
      "iter 4300: 0.001800\n",
      "iter 4400: 0.001797\n",
      "iter 4500: 0.001794\n",
      "iter 4600: 0.001791\n",
      "iter 4700: 0.001788\n",
      "iter 4800: 0.001786\n",
      "iter 4900: 0.001783\n",
      "iter 5000: 0.001781\n",
      "iter 5100: 0.001778\n",
      "iter 5200: 0.001776\n",
      "iter 5300: 0.001773\n",
      "iter 5400: 0.001771\n",
      "iter 5500: 0.001769\n",
      "iter 5600: 0.001767\n",
      "iter 5700: 0.001764\n",
      "iter 5800: 0.001762\n",
      "iter 5900: 0.001760\n",
      "iter 6000: 0.001758\n",
      "iter 6100: 0.001756\n",
      "iter 6200: 0.001754\n",
      "iter 6300: 0.001753\n",
      "iter 6400: 0.001751\n",
      "iter 6500: 0.001749\n",
      "iter 6600: 0.001747\n",
      "iter 6700: 0.001745\n",
      "iter 6800: 0.001744\n",
      "iter 6900: 0.001742\n",
      "iter 7000: 0.001740\n",
      "iter 7100: 0.001739\n",
      "iter 7200: 0.001737\n",
      "iter 7300: 0.001736\n",
      "iter 7400: 0.001734\n",
      "iter 7500: 0.001733\n",
      "iter 7600: 0.001731\n",
      "iter 7700: 0.001730\n",
      "iter 7800: 0.001729\n",
      "iter 7900: 0.001727\n",
      "iter 8000: 0.001726\n",
      "iter 8100: 0.001725\n",
      "iter 8200: 0.001724\n",
      "iter 8300: 0.001722\n",
      "iter 8400: 0.001721\n",
      "iter 8500: 0.001720\n",
      "iter 8600: 0.001719\n",
      "iter 8700: 0.001717\n",
      "iter 8800: 0.001716\n",
      "iter 8900: 0.001715\n",
      "iter 9000: 0.001714\n",
      "iter 9100: 0.001713\n",
      "iter 9200: 0.001712\n",
      "iter 9300: 0.001711\n",
      "iter 9400: 0.001710\n",
      "iter 9500: 0.001709\n",
      "iter 9600: 0.001708\n",
      "iter 9700: 0.001707\n",
      "iter 9800: 0.001706\n",
      "iter 9900: 0.001705\n",
      "iter 10000: 0.001704\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000017\n",
      "iter 100: 0.002320\n",
      "iter 200: 0.002319\n",
      "iter 300: 0.002318\n",
      "iter 400: 0.002315\n",
      "iter 500: 0.002313\n",
      "iter 600: 0.002309\n",
      "iter 700: 0.002306\n",
      "iter 800: 0.002302\n",
      "iter 900: 0.002298\n",
      "iter 1000: 0.002293\n",
      "iter 1100: 0.002289\n",
      "iter 1200: 0.002284\n",
      "iter 1300: 0.002279\n",
      "iter 1400: 0.002275\n",
      "iter 1500: 0.002270\n",
      "iter 1600: 0.002265\n",
      "iter 1700: 0.002260\n",
      "iter 1800: 0.002255\n",
      "iter 1900: 0.002250\n",
      "iter 2000: 0.002245\n",
      "iter 2100: 0.002240\n",
      "iter 2200: 0.002235\n",
      "iter 2300: 0.002231\n",
      "iter 2400: 0.002226\n",
      "iter 2500: 0.002221\n",
      "iter 2600: 0.002216\n",
      "iter 2700: 0.002212\n",
      "iter 2800: 0.002207\n",
      "iter 2900: 0.002202\n",
      "iter 3000: 0.002198\n",
      "iter 3100: 0.002193\n",
      "iter 3200: 0.002189\n",
      "iter 3300: 0.002184\n",
      "iter 3400: 0.002180\n",
      "iter 3500: 0.002176\n",
      "iter 3600: 0.002172\n",
      "iter 3700: 0.002167\n",
      "iter 3800: 0.002163\n",
      "iter 3900: 0.002159\n",
      "iter 4000: 0.002155\n",
      "iter 4100: 0.002151\n",
      "iter 4200: 0.002147\n",
      "iter 4300: 0.002143\n",
      "iter 4400: 0.002140\n",
      "iter 4500: 0.002136\n",
      "iter 4600: 0.002132\n",
      "iter 4700: 0.002129\n",
      "iter 4800: 0.002125\n",
      "iter 4900: 0.002121\n",
      "iter 5000: 0.002118\n",
      "iter 5100: 0.002115\n",
      "iter 5200: 0.002111\n",
      "iter 5300: 0.002108\n",
      "iter 5400: 0.002105\n",
      "iter 5500: 0.002101\n",
      "iter 5600: 0.002098\n",
      "iter 5700: 0.002095\n",
      "iter 5800: 0.002092\n",
      "iter 5900: 0.002089\n",
      "iter 6000: 0.002086\n",
      "iter 6100: 0.002083\n",
      "iter 6200: 0.002080\n",
      "iter 6300: 0.002077\n",
      "iter 6400: 0.002074\n",
      "iter 6500: 0.002071\n",
      "iter 6600: 0.002068\n",
      "iter 6700: 0.002066\n",
      "iter 6800: 0.002063\n",
      "iter 6900: 0.002060\n",
      "iter 7000: 0.002058\n",
      "iter 7100: 0.002055\n",
      "iter 7200: 0.002052\n",
      "iter 7300: 0.002050\n",
      "iter 7400: 0.002047\n",
      "iter 7500: 0.002045\n",
      "iter 7600: 0.002042\n",
      "iter 7700: 0.002040\n",
      "iter 7800: 0.002038\n",
      "iter 7900: 0.002035\n",
      "iter 8000: 0.002033\n",
      "iter 8100: 0.002031\n",
      "iter 8200: 0.002028\n",
      "iter 8300: 0.002026\n",
      "iter 8400: 0.002024\n",
      "iter 8500: 0.002022\n",
      "iter 8600: 0.002020\n",
      "iter 8700: 0.002018\n",
      "iter 8800: 0.002015\n",
      "iter 8900: 0.002013\n",
      "iter 9000: 0.002011\n",
      "iter 9100: 0.002009\n",
      "iter 9200: 0.002007\n",
      "iter 9300: 0.002005\n",
      "iter 9400: 0.002003\n",
      "iter 9500: 0.002001\n",
      "iter 9600: 0.002000\n",
      "iter 9700: 0.001998\n",
      "iter 9800: 0.001996\n",
      "iter 9900: 0.001994\n",
      "iter 10000: 0.001992\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000033\n",
      "iter 100: 0.003018\n",
      "iter 200: 0.003017\n",
      "iter 300: 0.003015\n",
      "iter 400: 0.003012\n",
      "iter 500: 0.003008\n",
      "iter 600: 0.003004\n",
      "iter 700: 0.002999\n",
      "iter 800: 0.002993\n",
      "iter 900: 0.002987\n",
      "iter 1000: 0.002981\n",
      "iter 1100: 0.002974\n",
      "iter 1200: 0.002966\n",
      "iter 1300: 0.002959\n",
      "iter 1400: 0.002951\n",
      "iter 1500: 0.002943\n",
      "iter 1600: 0.002934\n",
      "iter 1700: 0.002926\n",
      "iter 1800: 0.002917\n",
      "iter 1900: 0.002908\n",
      "iter 2000: 0.002899\n",
      "iter 2100: 0.002890\n",
      "iter 2200: 0.002881\n",
      "iter 2300: 0.002872\n",
      "iter 2400: 0.002862\n",
      "iter 2500: 0.002853\n",
      "iter 2600: 0.002844\n",
      "iter 2700: 0.002834\n",
      "iter 2800: 0.002825\n",
      "iter 2900: 0.002815\n",
      "iter 3000: 0.002806\n",
      "iter 3100: 0.002797\n",
      "iter 3200: 0.002787\n",
      "iter 3300: 0.002778\n",
      "iter 3400: 0.002769\n",
      "iter 3500: 0.002760\n",
      "iter 3600: 0.002751\n",
      "iter 3700: 0.002742\n",
      "iter 3800: 0.002733\n",
      "iter 3900: 0.002724\n",
      "iter 4000: 0.002715\n",
      "iter 4100: 0.002706\n",
      "iter 4200: 0.002698\n",
      "iter 4300: 0.002689\n",
      "iter 4400: 0.002681\n",
      "iter 4500: 0.002672\n",
      "iter 4600: 0.002664\n",
      "iter 4700: 0.002656\n",
      "iter 4800: 0.002648\n",
      "iter 4900: 0.002640\n",
      "iter 5000: 0.002632\n",
      "iter 5100: 0.002625\n",
      "iter 5200: 0.002617\n",
      "iter 5300: 0.002609\n",
      "iter 5400: 0.002602\n",
      "iter 5500: 0.002595\n",
      "iter 5600: 0.002588\n",
      "iter 5700: 0.002580\n",
      "iter 5800: 0.002573\n",
      "iter 5900: 0.002567\n",
      "iter 6000: 0.002560\n",
      "iter 6100: 0.002553\n",
      "iter 6200: 0.002547\n",
      "iter 6300: 0.002540\n",
      "iter 6400: 0.002534\n",
      "iter 6500: 0.002528\n",
      "iter 6600: 0.002521\n",
      "iter 6700: 0.002515\n",
      "iter 6800: 0.002509\n",
      "iter 6900: 0.002504\n",
      "iter 7000: 0.002498\n",
      "iter 7100: 0.002492\n",
      "iter 7200: 0.002487\n",
      "iter 7300: 0.002481\n",
      "iter 7400: 0.002476\n",
      "iter 7500: 0.002471\n",
      "iter 7600: 0.002466\n",
      "iter 7700: 0.002460\n",
      "iter 7800: 0.002455\n",
      "iter 7900: 0.002451\n",
      "iter 8000: 0.002446\n",
      "iter 8100: 0.002441\n",
      "iter 8200: 0.002436\n",
      "iter 8300: 0.002432\n",
      "iter 8400: 0.002427\n",
      "iter 8500: 0.002423\n",
      "iter 8600: 0.002419\n",
      "iter 8700: 0.002415\n",
      "iter 8800: 0.002410\n",
      "iter 8900: 0.002406\n",
      "iter 9000: 0.002402\n",
      "iter 9100: 0.002399\n",
      "iter 9200: 0.002395\n",
      "iter 9300: 0.002391\n",
      "iter 9400: 0.002387\n",
      "iter 9500: 0.002384\n",
      "iter 9600: 0.002380\n",
      "iter 9700: 0.002377\n",
      "iter 9800: 0.002373\n",
      "iter 9900: 0.002370\n",
      "iter 10000: 0.002367\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000068\n",
      "iter 100: 0.004386\n",
      "iter 200: 0.004383\n",
      "iter 300: 0.004377\n",
      "iter 400: 0.004368\n",
      "iter 500: 0.004356\n",
      "iter 600: 0.004343\n",
      "iter 700: 0.004327\n",
      "iter 800: 0.004310\n",
      "iter 900: 0.004291\n",
      "iter 1000: 0.004271\n",
      "iter 1100: 0.004249\n",
      "iter 1200: 0.004226\n",
      "iter 1300: 0.004203\n",
      "iter 1400: 0.004179\n",
      "iter 1500: 0.004154\n",
      "iter 1600: 0.004128\n",
      "iter 1700: 0.004102\n",
      "iter 1800: 0.004076\n",
      "iter 1900: 0.004049\n",
      "iter 2000: 0.004023\n",
      "iter 2100: 0.003996\n",
      "iter 2200: 0.003969\n",
      "iter 2300: 0.003942\n",
      "iter 2400: 0.003915\n",
      "iter 2500: 0.003888\n",
      "iter 2600: 0.003862\n",
      "iter 2700: 0.003835\n",
      "iter 2800: 0.003809\n",
      "iter 2900: 0.003783\n",
      "iter 3000: 0.003758\n",
      "iter 3100: 0.003733\n",
      "iter 3200: 0.003708\n",
      "iter 3300: 0.003683\n",
      "iter 3400: 0.003659\n",
      "iter 3500: 0.003635\n",
      "iter 3600: 0.003612\n",
      "iter 3700: 0.003589\n",
      "iter 3800: 0.003567\n",
      "iter 3900: 0.003545\n",
      "iter 4000: 0.003523\n",
      "iter 4100: 0.003502\n",
      "iter 4200: 0.003482\n",
      "iter 4300: 0.003462\n",
      "iter 4400: 0.003442\n",
      "iter 4500: 0.003423\n",
      "iter 4600: 0.003404\n",
      "iter 4700: 0.003386\n",
      "iter 4800: 0.003368\n",
      "iter 4900: 0.003350\n",
      "iter 5000: 0.003333\n",
      "iter 5100: 0.003317\n",
      "iter 5200: 0.003301\n",
      "iter 5300: 0.003285\n",
      "iter 5400: 0.003270\n",
      "iter 5500: 0.003255\n",
      "iter 5600: 0.003241\n",
      "iter 5700: 0.003227\n",
      "iter 5800: 0.003213\n",
      "iter 5900: 0.003200\n",
      "iter 6000: 0.003187\n",
      "iter 6100: 0.003175\n",
      "iter 6200: 0.003163\n",
      "iter 6300: 0.003151\n",
      "iter 6400: 0.003140\n",
      "iter 6500: 0.003129\n",
      "iter 6600: 0.003118\n",
      "iter 6700: 0.003108\n",
      "iter 6800: 0.003098\n",
      "iter 6900: 0.003088\n",
      "iter 7000: 0.003078\n",
      "iter 7100: 0.003069\n",
      "iter 7200: 0.003060\n",
      "iter 7300: 0.003052\n",
      "iter 7400: 0.003044\n",
      "iter 7500: 0.003036\n",
      "iter 7600: 0.003028\n",
      "iter 7700: 0.003020\n",
      "iter 7800: 0.003013\n",
      "iter 7900: 0.003006\n",
      "iter 8000: 0.002999\n",
      "iter 8100: 0.002993\n",
      "iter 8200: 0.002986\n",
      "iter 8300: 0.002980\n",
      "iter 8400: 0.002974\n",
      "iter 8500: 0.002968\n",
      "iter 8600: 0.002963\n",
      "iter 8700: 0.002957\n",
      "iter 8800: 0.002952\n",
      "iter 8900: 0.002947\n",
      "iter 9000: 0.002942\n",
      "iter 9100: 0.002938\n",
      "iter 9200: 0.002933\n",
      "iter 9300: 0.002929\n",
      "iter 9400: 0.002924\n",
      "iter 9500: 0.002920\n",
      "iter 9600: 0.002916\n",
      "iter 9700: 0.002913\n",
      "iter 9800: 0.002909\n",
      "iter 9900: 0.002905\n",
      "iter 10000: 0.002902\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000136\n",
      "iter 100: 0.006989\n",
      "iter 200: 0.006975\n",
      "iter 300: 0.006949\n",
      "iter 400: 0.006912\n",
      "iter 500: 0.006867\n",
      "iter 600: 0.006815\n",
      "iter 700: 0.006757\n",
      "iter 800: 0.006694\n",
      "iter 900: 0.006628\n",
      "iter 1000: 0.006558\n",
      "iter 1100: 0.006485\n",
      "iter 1200: 0.006411\n",
      "iter 1300: 0.006336\n",
      "iter 1400: 0.006260\n",
      "iter 1500: 0.006183\n",
      "iter 1600: 0.006107\n",
      "iter 1700: 0.006031\n",
      "iter 1800: 0.005955\n",
      "iter 1900: 0.005880\n",
      "iter 2000: 0.005807\n",
      "iter 2100: 0.005734\n",
      "iter 2200: 0.005663\n",
      "iter 2300: 0.005593\n",
      "iter 2400: 0.005525\n",
      "iter 2500: 0.005459\n",
      "iter 2600: 0.005394\n",
      "iter 2700: 0.005331\n",
      "iter 2800: 0.005270\n",
      "iter 2900: 0.005211\n",
      "iter 3000: 0.005153\n",
      "iter 3100: 0.005097\n",
      "iter 3200: 0.005043\n",
      "iter 3300: 0.004991\n",
      "iter 3400: 0.004941\n",
      "iter 3500: 0.004892\n",
      "iter 3600: 0.004846\n",
      "iter 3700: 0.004801\n",
      "iter 3800: 0.004757\n",
      "iter 3900: 0.004715\n",
      "iter 4000: 0.004675\n",
      "iter 4100: 0.004636\n",
      "iter 4200: 0.004599\n",
      "iter 4300: 0.004563\n",
      "iter 4400: 0.004529\n",
      "iter 4500: 0.004496\n",
      "iter 4600: 0.004465\n",
      "iter 4700: 0.004434\n",
      "iter 4800: 0.004405\n",
      "iter 4900: 0.004378\n",
      "iter 5000: 0.004351\n",
      "iter 5100: 0.004326\n",
      "iter 5200: 0.004301\n",
      "iter 5300: 0.004278\n",
      "iter 5400: 0.004256\n",
      "iter 5500: 0.004234\n",
      "iter 5600: 0.004214\n",
      "iter 5700: 0.004194\n",
      "iter 5800: 0.004176\n",
      "iter 5900: 0.004158\n",
      "iter 6000: 0.004141\n",
      "iter 6100: 0.004124\n",
      "iter 6200: 0.004109\n",
      "iter 6300: 0.004094\n",
      "iter 6400: 0.004080\n",
      "iter 6500: 0.004066\n",
      "iter 6600: 0.004053\n",
      "iter 6700: 0.004041\n",
      "iter 6800: 0.004029\n",
      "iter 6900: 0.004018\n",
      "iter 7000: 0.004007\n",
      "iter 7100: 0.003997\n",
      "iter 7200: 0.003988\n",
      "iter 7300: 0.003978\n",
      "iter 7400: 0.003969\n",
      "iter 7500: 0.003961\n",
      "iter 7600: 0.003953\n",
      "iter 7700: 0.003945\n",
      "iter 7800: 0.003938\n",
      "iter 7900: 0.003931\n",
      "iter 8000: 0.003924\n",
      "iter 8100: 0.003918\n",
      "iter 8200: 0.003912\n",
      "iter 8300: 0.003906\n",
      "iter 8400: 0.003901\n",
      "iter 8500: 0.003896\n",
      "iter 8600: 0.003891\n",
      "iter 8700: 0.003886\n",
      "iter 8800: 0.003882\n",
      "iter 8900: 0.003877\n",
      "iter 9000: 0.003873\n",
      "iter 9100: 0.003870\n",
      "iter 9200: 0.003866\n",
      "iter 9300: 0.003862\n",
      "iter 9400: 0.003859\n",
      "iter 9500: 0.003856\n",
      "iter 9600: 0.003853\n",
      "iter 9700: 0.003850\n",
      "iter 9800: 0.003847\n",
      "iter 9900: 0.003845\n",
      "iter 10000: 0.003842\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000275\n",
      "iter 100: 0.011648\n",
      "iter 200: 0.011591\n",
      "iter 300: 0.011493\n",
      "iter 400: 0.011364\n",
      "iter 500: 0.011213\n",
      "iter 600: 0.011046\n",
      "iter 700: 0.010867\n",
      "iter 800: 0.010682\n",
      "iter 900: 0.010491\n",
      "iter 1000: 0.010299\n",
      "iter 1100: 0.010107\n",
      "iter 1200: 0.009916\n",
      "iter 1300: 0.009728\n",
      "iter 1400: 0.009544\n",
      "iter 1500: 0.009363\n",
      "iter 1600: 0.009188\n",
      "iter 1700: 0.009019\n",
      "iter 1800: 0.008854\n",
      "iter 1900: 0.008696\n",
      "iter 2000: 0.008543\n",
      "iter 2100: 0.008397\n",
      "iter 2200: 0.008256\n",
      "iter 2300: 0.008121\n",
      "iter 2400: 0.007991\n",
      "iter 2500: 0.007867\n",
      "iter 2600: 0.007749\n",
      "iter 2700: 0.007636\n",
      "iter 2800: 0.007528\n",
      "iter 2900: 0.007425\n",
      "iter 3000: 0.007327\n",
      "iter 3100: 0.007233\n",
      "iter 3200: 0.007143\n",
      "iter 3300: 0.007058\n",
      "iter 3400: 0.006977\n",
      "iter 3500: 0.006900\n",
      "iter 3600: 0.006827\n",
      "iter 3700: 0.006757\n",
      "iter 3800: 0.006690\n",
      "iter 3900: 0.006627\n",
      "iter 4000: 0.006567\n",
      "iter 4100: 0.006510\n",
      "iter 4200: 0.006455\n",
      "iter 4300: 0.006403\n",
      "iter 4400: 0.006354\n",
      "iter 4500: 0.006307\n",
      "iter 4600: 0.006263\n",
      "iter 4700: 0.006221\n",
      "iter 4800: 0.006181\n",
      "iter 4900: 0.006142\n",
      "iter 5000: 0.006106\n",
      "iter 5100: 0.006072\n",
      "iter 5200: 0.006039\n",
      "iter 5300: 0.006008\n",
      "iter 5400: 0.005978\n",
      "iter 5500: 0.005950\n",
      "iter 5600: 0.005923\n",
      "iter 5700: 0.005898\n",
      "iter 5800: 0.005874\n",
      "iter 5900: 0.005851\n",
      "iter 6000: 0.005829\n",
      "iter 6100: 0.005808\n",
      "iter 6200: 0.005789\n",
      "iter 6300: 0.005770\n",
      "iter 6400: 0.005752\n",
      "iter 6500: 0.005735\n",
      "iter 6600: 0.005719\n",
      "iter 6700: 0.005704\n",
      "iter 6800: 0.005690\n",
      "iter 6900: 0.005676\n",
      "iter 7000: 0.005663\n",
      "iter 7100: 0.005651\n",
      "iter 7200: 0.005639\n",
      "iter 7300: 0.005628\n",
      "iter 7400: 0.005617\n",
      "iter 7500: 0.005607\n",
      "iter 7600: 0.005597\n",
      "iter 7700: 0.005588\n",
      "iter 7800: 0.005579\n",
      "iter 7900: 0.005571\n",
      "iter 8000: 0.005563\n",
      "iter 8100: 0.005556\n",
      "iter 8200: 0.005549\n",
      "iter 8300: 0.005542\n",
      "iter 8400: 0.005536\n",
      "iter 8500: 0.005530\n",
      "iter 8600: 0.005524\n",
      "iter 8700: 0.005519\n",
      "iter 8800: 0.005513\n",
      "iter 8900: 0.005509\n",
      "iter 9000: 0.005504\n",
      "iter 9100: 0.005499\n",
      "iter 9200: 0.005495\n",
      "iter 9300: 0.005491\n",
      "iter 9400: 0.005487\n",
      "iter 9500: 0.005484\n",
      "iter 9600: 0.005480\n",
      "iter 9700: 0.005477\n",
      "iter 9800: 0.005474\n",
      "iter 9900: 0.005471\n",
      "iter 10000: 0.005468\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.000556\n",
      "iter 100: 0.018980\n",
      "iter 200: 0.018798\n",
      "iter 300: 0.018514\n",
      "iter 400: 0.018168\n",
      "iter 500: 0.017789\n",
      "iter 600: 0.017392\n",
      "iter 700: 0.016990\n",
      "iter 800: 0.016590\n",
      "iter 900: 0.016198\n",
      "iter 1000: 0.015815\n",
      "iter 1100: 0.015446\n",
      "iter 1200: 0.015091\n",
      "iter 1300: 0.014750\n",
      "iter 1400: 0.014423\n",
      "iter 1500: 0.014112\n",
      "iter 1600: 0.013814\n",
      "iter 1700: 0.013531\n",
      "iter 1800: 0.013261\n",
      "iter 1900: 0.013005\n",
      "iter 2000: 0.012760\n",
      "iter 2100: 0.012528\n",
      "iter 2200: 0.012307\n",
      "iter 2300: 0.012098\n",
      "iter 2400: 0.011898\n",
      "iter 2500: 0.011709\n",
      "iter 2600: 0.011529\n",
      "iter 2700: 0.011358\n",
      "iter 2800: 0.011195\n",
      "iter 2900: 0.011041\n",
      "iter 3000: 0.010894\n",
      "iter 3100: 0.010754\n",
      "iter 3200: 0.010622\n",
      "iter 3300: 0.010496\n",
      "iter 3400: 0.010377\n",
      "iter 3500: 0.010263\n",
      "iter 3600: 0.010155\n",
      "iter 3700: 0.010053\n",
      "iter 3800: 0.009955\n",
      "iter 3900: 0.009863\n",
      "iter 4000: 0.009775\n",
      "iter 4100: 0.009691\n",
      "iter 4200: 0.009612\n",
      "iter 4300: 0.009537\n",
      "iter 4400: 0.009465\n",
      "iter 4500: 0.009397\n",
      "iter 4600: 0.009333\n",
      "iter 4700: 0.009271\n",
      "iter 4800: 0.009213\n",
      "iter 4900: 0.009158\n",
      "iter 5000: 0.009105\n",
      "iter 5100: 0.009055\n",
      "iter 5200: 0.009008\n",
      "iter 5300: 0.008962\n",
      "iter 5400: 0.008920\n",
      "iter 5500: 0.008879\n",
      "iter 5600: 0.008840\n",
      "iter 5700: 0.008803\n",
      "iter 5800: 0.008769\n",
      "iter 5900: 0.008735\n",
      "iter 6000: 0.008704\n",
      "iter 6100: 0.008674\n",
      "iter 6200: 0.008646\n",
      "iter 6300: 0.008619\n",
      "iter 6400: 0.008593\n",
      "iter 6500: 0.008568\n",
      "iter 6600: 0.008545\n",
      "iter 6700: 0.008523\n",
      "iter 6800: 0.008502\n",
      "iter 6900: 0.008483\n",
      "iter 7000: 0.008464\n",
      "iter 7100: 0.008446\n",
      "iter 7200: 0.008429\n",
      "iter 7300: 0.008413\n",
      "iter 7400: 0.008397\n",
      "iter 7500: 0.008383\n",
      "iter 7600: 0.008369\n",
      "iter 7700: 0.008356\n",
      "iter 7800: 0.008343\n",
      "iter 7900: 0.008331\n",
      "iter 8000: 0.008320\n",
      "iter 8100: 0.008309\n",
      "iter 8200: 0.008299\n",
      "iter 8300: 0.008289\n",
      "iter 8400: 0.008280\n",
      "iter 8500: 0.008271\n",
      "iter 8600: 0.008263\n",
      "iter 8700: 0.008255\n",
      "iter 8800: 0.008248\n",
      "iter 8900: 0.008241\n",
      "iter 9000: 0.008234\n",
      "iter 9100: 0.008227\n",
      "iter 9200: 0.008221\n",
      "iter 9300: 0.008215\n",
      "iter 9400: 0.008210\n",
      "iter 9500: 0.008205\n",
      "iter 9600: 0.008200\n",
      "iter 9700: 0.008195\n",
      "iter 9800: 0.008191\n",
      "iter 9900: 0.008186\n",
      "iter 10000: 0.008182\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.001122\n",
      "iter 100: 0.027797\n",
      "iter 200: 0.027391\n",
      "iter 300: 0.026825\n",
      "iter 400: 0.026200\n",
      "iter 500: 0.025563\n",
      "iter 600: 0.024937\n",
      "iter 700: 0.024330\n",
      "iter 800: 0.023748\n",
      "iter 900: 0.023193\n",
      "iter 1000: 0.022663\n",
      "iter 1100: 0.022160\n",
      "iter 1200: 0.021681\n",
      "iter 1300: 0.021226\n",
      "iter 1400: 0.020794\n",
      "iter 1500: 0.020383\n",
      "iter 1600: 0.019992\n",
      "iter 1700: 0.019622\n",
      "iter 1800: 0.019269\n",
      "iter 1900: 0.018935\n",
      "iter 2000: 0.018617\n",
      "iter 2100: 0.018315\n",
      "iter 2200: 0.018028\n",
      "iter 2300: 0.017755\n",
      "iter 2400: 0.017496\n",
      "iter 2500: 0.017250\n",
      "iter 2600: 0.017017\n",
      "iter 2700: 0.016795\n",
      "iter 2800: 0.016584\n",
      "iter 2900: 0.016383\n",
      "iter 3000: 0.016193\n",
      "iter 3100: 0.016012\n",
      "iter 3200: 0.015841\n",
      "iter 3300: 0.015677\n",
      "iter 3400: 0.015522\n",
      "iter 3500: 0.015375\n",
      "iter 3600: 0.015235\n",
      "iter 3700: 0.015102\n",
      "iter 3800: 0.014976\n",
      "iter 3900: 0.014856\n",
      "iter 4000: 0.014742\n",
      "iter 4100: 0.014634\n",
      "iter 4200: 0.014531\n",
      "iter 4300: 0.014433\n",
      "iter 4400: 0.014340\n",
      "iter 4500: 0.014252\n",
      "iter 4600: 0.014168\n",
      "iter 4700: 0.014089\n",
      "iter 4800: 0.014013\n",
      "iter 4900: 0.013941\n",
      "iter 5000: 0.013873\n",
      "iter 5100: 0.013808\n",
      "iter 5200: 0.013747\n",
      "iter 5300: 0.013688\n",
      "iter 5400: 0.013633\n",
      "iter 5500: 0.013580\n",
      "iter 5600: 0.013530\n",
      "iter 5700: 0.013482\n",
      "iter 5800: 0.013437\n",
      "iter 5900: 0.013394\n",
      "iter 6000: 0.013353\n",
      "iter 6100: 0.013314\n",
      "iter 6200: 0.013277\n",
      "iter 6300: 0.013242\n",
      "iter 6400: 0.013209\n",
      "iter 6500: 0.013177\n",
      "iter 6600: 0.013147\n",
      "iter 6700: 0.013119\n",
      "iter 6800: 0.013092\n",
      "iter 6900: 0.013066\n",
      "iter 7000: 0.013041\n",
      "iter 7100: 0.013018\n",
      "iter 7200: 0.012996\n",
      "iter 7300: 0.012975\n",
      "iter 7400: 0.012955\n",
      "iter 7500: 0.012936\n",
      "iter 7600: 0.012918\n",
      "iter 7700: 0.012901\n",
      "iter 7800: 0.012885\n",
      "iter 7900: 0.012869\n",
      "iter 8000: 0.012855\n",
      "iter 8100: 0.012841\n",
      "iter 8200: 0.012828\n",
      "iter 8300: 0.012815\n",
      "iter 8400: 0.012803\n",
      "iter 8500: 0.012792\n",
      "iter 8600: 0.012781\n",
      "iter 8700: 0.012771\n",
      "iter 8800: 0.012761\n",
      "iter 8900: 0.012752\n",
      "iter 9000: 0.012743\n",
      "iter 9100: 0.012735\n",
      "iter 9200: 0.012727\n",
      "iter 9300: 0.012719\n",
      "iter 9400: 0.012712\n",
      "iter 9500: 0.012705\n",
      "iter 9600: 0.012699\n",
      "iter 9700: 0.012693\n",
      "iter 9800: 0.012687\n",
      "iter 9900: 0.012682\n",
      "iter 10000: 0.012676\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.002265\n",
      "iter 100: 0.034418\n",
      "iter 200: 0.033870\n",
      "iter 300: 0.033219\n",
      "iter 400: 0.032568\n",
      "iter 500: 0.031942\n",
      "iter 600: 0.031345\n",
      "iter 700: 0.030776\n",
      "iter 800: 0.030237\n",
      "iter 900: 0.029724\n",
      "iter 1000: 0.029237\n",
      "iter 1100: 0.028774\n",
      "iter 1200: 0.028334\n",
      "iter 1300: 0.027916\n",
      "iter 1400: 0.027520\n",
      "iter 1500: 0.027143\n",
      "iter 1600: 0.026784\n",
      "iter 1700: 0.026444\n",
      "iter 1800: 0.026121\n",
      "iter 1900: 0.025814\n",
      "iter 2000: 0.025522\n",
      "iter 2100: 0.025245\n",
      "iter 2200: 0.024982\n",
      "iter 2300: 0.024732\n",
      "iter 2400: 0.024494\n",
      "iter 2500: 0.024268\n",
      "iter 2600: 0.024054\n",
      "iter 2700: 0.023850\n",
      "iter 2800: 0.023657\n",
      "iter 2900: 0.023473\n",
      "iter 3000: 0.023298\n",
      "iter 3100: 0.023132\n",
      "iter 3200: 0.022975\n",
      "iter 3300: 0.022825\n",
      "iter 3400: 0.022683\n",
      "iter 3500: 0.022547\n",
      "iter 3600: 0.022419\n",
      "iter 3700: 0.022297\n",
      "iter 3800: 0.022181\n",
      "iter 3900: 0.022071\n",
      "iter 4000: 0.021967\n",
      "iter 4100: 0.021867\n",
      "iter 4200: 0.021773\n",
      "iter 4300: 0.021683\n",
      "iter 4400: 0.021598\n",
      "iter 4500: 0.021517\n",
      "iter 4600: 0.021440\n",
      "iter 4700: 0.021367\n",
      "iter 4800: 0.021298\n",
      "iter 4900: 0.021232\n",
      "iter 5000: 0.021169\n",
      "iter 5100: 0.021110\n",
      "iter 5200: 0.021053\n",
      "iter 5300: 0.021000\n",
      "iter 5400: 0.020949\n",
      "iter 5500: 0.020900\n",
      "iter 5600: 0.020854\n",
      "iter 5700: 0.020810\n",
      "iter 5800: 0.020769\n",
      "iter 5900: 0.020729\n",
      "iter 6000: 0.020692\n",
      "iter 6100: 0.020656\n",
      "iter 6200: 0.020623\n",
      "iter 6300: 0.020590\n",
      "iter 6400: 0.020560\n",
      "iter 6500: 0.020531\n",
      "iter 6600: 0.020503\n",
      "iter 6700: 0.020477\n",
      "iter 6800: 0.020452\n",
      "iter 6900: 0.020429\n",
      "iter 7000: 0.020406\n",
      "iter 7100: 0.020385\n",
      "iter 7200: 0.020365\n",
      "iter 7300: 0.020345\n",
      "iter 7400: 0.020327\n",
      "iter 7500: 0.020310\n",
      "iter 7600: 0.020293\n",
      "iter 7700: 0.020277\n",
      "iter 7800: 0.020263\n",
      "iter 7900: 0.020248\n",
      "iter 8000: 0.020235\n",
      "iter 8100: 0.020222\n",
      "iter 8200: 0.020210\n",
      "iter 8300: 0.020199\n",
      "iter 8400: 0.020188\n",
      "iter 8500: 0.020177\n",
      "iter 8600: 0.020167\n",
      "iter 8700: 0.020158\n",
      "iter 8800: 0.020149\n",
      "iter 8900: 0.020141\n",
      "iter 9000: 0.020133\n",
      "iter 9100: 0.020125\n",
      "iter 9200: 0.020118\n",
      "iter 9300: 0.020111\n",
      "iter 9400: 0.020104\n",
      "iter 9500: 0.020098\n",
      "iter 9600: 0.020092\n",
      "iter 9700: 0.020086\n",
      "iter 9800: 0.020081\n",
      "iter 9900: 0.020076\n",
      "iter 10000: 0.020071\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.004571\n",
      "iter 100: 0.039071\n",
      "iter 200: 0.038746\n",
      "iter 300: 0.038417\n",
      "iter 400: 0.038103\n",
      "iter 500: 0.037805\n",
      "iter 600: 0.037522\n",
      "iter 700: 0.037253\n",
      "iter 800: 0.036998\n",
      "iter 900: 0.036755\n",
      "iter 1000: 0.036524\n",
      "iter 1100: 0.036305\n",
      "iter 1200: 0.036097\n",
      "iter 1300: 0.035899\n",
      "iter 1400: 0.035712\n",
      "iter 1500: 0.035533\n",
      "iter 1600: 0.035364\n",
      "iter 1700: 0.035203\n",
      "iter 1800: 0.035050\n",
      "iter 1900: 0.034904\n",
      "iter 2000: 0.034766\n",
      "iter 2100: 0.034635\n",
      "iter 2200: 0.034510\n",
      "iter 2300: 0.034392\n",
      "iter 2400: 0.034280\n",
      "iter 2500: 0.034173\n",
      "iter 2600: 0.034071\n",
      "iter 2700: 0.033975\n",
      "iter 2800: 0.033883\n",
      "iter 2900: 0.033796\n",
      "iter 3000: 0.033714\n",
      "iter 3100: 0.033635\n",
      "iter 3200: 0.033560\n",
      "iter 3300: 0.033490\n",
      "iter 3400: 0.033422\n",
      "iter 3500: 0.033358\n",
      "iter 3600: 0.033297\n",
      "iter 3700: 0.033240\n",
      "iter 3800: 0.033185\n",
      "iter 3900: 0.033133\n",
      "iter 4000: 0.033083\n",
      "iter 4100: 0.033036\n",
      "iter 4200: 0.032992\n",
      "iter 4300: 0.032949\n",
      "iter 4400: 0.032909\n",
      "iter 4500: 0.032871\n",
      "iter 4600: 0.032834\n",
      "iter 4700: 0.032800\n",
      "iter 4800: 0.032767\n",
      "iter 4900: 0.032736\n",
      "iter 5000: 0.032706\n",
      "iter 5100: 0.032678\n",
      "iter 5200: 0.032651\n",
      "iter 5300: 0.032626\n",
      "iter 5400: 0.032602\n",
      "iter 5500: 0.032579\n",
      "iter 5600: 0.032557\n",
      "iter 5700: 0.032536\n",
      "iter 5800: 0.032516\n",
      "iter 5900: 0.032498\n",
      "iter 6000: 0.032480\n",
      "iter 6100: 0.032463\n",
      "iter 6200: 0.032447\n",
      "iter 6300: 0.032432\n",
      "iter 6400: 0.032417\n",
      "iter 6500: 0.032404\n",
      "iter 6600: 0.032391\n",
      "iter 6700: 0.032378\n",
      "iter 6800: 0.032367\n",
      "iter 6900: 0.032355\n",
      "iter 7000: 0.032345\n",
      "iter 7100: 0.032335\n",
      "iter 7200: 0.032325\n",
      "iter 7300: 0.032316\n",
      "iter 7400: 0.032307\n",
      "iter 7500: 0.032299\n",
      "iter 7600: 0.032291\n",
      "iter 7700: 0.032284\n",
      "iter 7800: 0.032277\n",
      "iter 7900: 0.032270\n",
      "iter 8000: 0.032264\n",
      "iter 8100: 0.032258\n",
      "iter 8200: 0.032252\n",
      "iter 8300: 0.032247\n",
      "iter 8400: 0.032241\n",
      "iter 8500: 0.032236\n",
      "iter 8600: 0.032232\n",
      "iter 8700: 0.032227\n",
      "iter 8800: 0.032223\n",
      "iter 8900: 0.032219\n",
      "iter 9000: 0.032215\n",
      "iter 9100: 0.032212\n",
      "iter 9200: 0.032208\n",
      "iter 9300: 0.032205\n",
      "iter 9400: 0.032202\n",
      "iter 9500: 0.032199\n",
      "iter 9600: 0.032196\n",
      "iter 9700: 0.032193\n",
      "iter 9800: 0.032191\n",
      "iter 9900: 0.032189\n",
      "iter 10000: 0.032186\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.009226\n",
      "iter 100: 0.052711\n",
      "iter 200: 0.052670\n",
      "iter 300: 0.052632\n",
      "iter 400: 0.052595\n",
      "iter 500: 0.052560\n",
      "iter 600: 0.052527\n",
      "iter 700: 0.052496\n",
      "iter 800: 0.052466\n",
      "iter 900: 0.052438\n",
      "iter 1000: 0.052411\n",
      "iter 1100: 0.052385\n",
      "iter 1200: 0.052361\n",
      "iter 1300: 0.052338\n",
      "iter 1400: 0.052316\n",
      "iter 1500: 0.052295\n",
      "iter 1600: 0.052276\n",
      "iter 1700: 0.052257\n",
      "iter 1800: 0.052239\n",
      "iter 1900: 0.052222\n",
      "iter 2000: 0.052206\n",
      "iter 2100: 0.052191\n",
      "iter 2200: 0.052176\n",
      "iter 2300: 0.052162\n",
      "iter 2400: 0.052149\n",
      "iter 2500: 0.052137\n",
      "iter 2600: 0.052125\n",
      "iter 2700: 0.052114\n",
      "iter 2800: 0.052103\n",
      "iter 2900: 0.052093\n",
      "iter 3000: 0.052083\n",
      "iter 3100: 0.052074\n",
      "iter 3200: 0.052065\n",
      "iter 3300: 0.052057\n",
      "iter 3400: 0.052049\n",
      "iter 3500: 0.052042\n",
      "iter 3600: 0.052035\n",
      "iter 3700: 0.052028\n",
      "iter 3800: 0.052021\n",
      "iter 3900: 0.052015\n",
      "iter 4000: 0.052010\n",
      "iter 4100: 0.052004\n",
      "iter 4200: 0.051999\n",
      "iter 4300: 0.051994\n",
      "iter 4400: 0.051989\n",
      "iter 4500: 0.051985\n",
      "iter 4600: 0.051981\n",
      "iter 4700: 0.051976\n",
      "iter 4800: 0.051973\n",
      "iter 4900: 0.051969\n",
      "iter 5000: 0.051966\n",
      "iter 5100: 0.051962\n",
      "iter 5200: 0.051959\n",
      "iter 5300: 0.051956\n",
      "iter 5400: 0.051953\n",
      "iter 5500: 0.051951\n",
      "iter 5600: 0.051948\n",
      "iter 5700: 0.051946\n",
      "iter 5800: 0.051943\n",
      "iter 5900: 0.051941\n",
      "iter 6000: 0.051939\n",
      "iter 6100: 0.051937\n",
      "iter 6200: 0.051935\n",
      "iter 6300: 0.051934\n",
      "iter 6400: 0.051932\n",
      "iter 6500: 0.051930\n",
      "iter 6600: 0.051929\n",
      "iter 6700: 0.051927\n",
      "iter 6800: 0.051926\n",
      "iter 6900: 0.051925\n",
      "iter 7000: 0.051923\n",
      "iter 7100: 0.051922\n",
      "iter 7200: 0.051921\n",
      "iter 7300: 0.051920\n",
      "iter 7400: 0.051919\n",
      "iter 7500: 0.051918\n",
      "iter 7600: 0.051917\n",
      "iter 7700: 0.051916\n",
      "iter 7800: 0.051915\n",
      "iter 7900: 0.051915\n",
      "iter 8000: 0.051914\n",
      "iter 8100: 0.051913\n",
      "iter 8200: 0.051913\n",
      "iter 8300: 0.051912\n",
      "iter 8400: 0.051911\n",
      "iter 8500: 0.051911\n",
      "iter 8600: 0.051910\n",
      "iter 8700: 0.051910\n",
      "iter 8800: 0.051909\n",
      "iter 8900: 0.051909\n",
      "iter 9000: 0.051908\n",
      "iter 9100: 0.051908\n",
      "iter 9200: 0.051907\n",
      "iter 9300: 0.051907\n",
      "iter 9400: 0.051907\n",
      "iter 9500: 0.051906\n",
      "iter 9600: 0.051906\n",
      "iter 9700: 0.051906\n",
      "iter 9800: 0.051905\n",
      "iter 9900: 0.051905\n",
      "iter 10000: 0.051905\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.018621\n",
      "iter 100: 0.083506\n",
      "iter 200: 0.083506\n",
      "iter 300: 0.083506\n",
      "iter 400: 0.083506\n",
      "iter 500: 0.083506\n",
      "iter 600: 0.083505\n",
      "iter 700: 0.083505\n",
      "iter 800: 0.083505\n",
      "iter 900: 0.083505\n",
      "iter 1000: 0.083505\n",
      "iter 1100: 0.083505\n",
      "iter 1200: 0.083504\n",
      "iter 1300: 0.083504\n",
      "iter 1400: 0.083504\n",
      "iter 1500: 0.083504\n",
      "iter 1600: 0.083504\n",
      "iter 1700: 0.083504\n",
      "iter 1800: 0.083504\n",
      "iter 1900: 0.083504\n",
      "iter 2000: 0.083503\n",
      "iter 2100: 0.083503\n",
      "iter 2200: 0.083503\n",
      "iter 2300: 0.083503\n",
      "iter 2400: 0.083503\n",
      "iter 2500: 0.083503\n",
      "iter 2600: 0.083503\n",
      "iter 2700: 0.083503\n",
      "iter 2800: 0.083503\n",
      "iter 2900: 0.083503\n",
      "iter 3000: 0.083503\n",
      "iter 3100: 0.083503\n",
      "iter 3200: 0.083503\n",
      "iter 3300: 0.083503\n",
      "iter 3400: 0.083503\n",
      "iter 3500: 0.083502\n",
      "iter 3600: 0.083502\n",
      "iter 3700: 0.083502\n",
      "iter 3800: 0.083502\n",
      "iter 3900: 0.083502\n",
      "iter 4000: 0.083502\n",
      "iter 4100: 0.083502\n",
      "iter 4200: 0.083502\n",
      "iter 4300: 0.083502\n",
      "iter 4400: 0.083502\n",
      "iter 4500: 0.083502\n",
      "iter 4600: 0.083502\n",
      "iter 4700: 0.083502\n",
      "iter 4800: 0.083502\n",
      "iter 4900: 0.083502\n",
      "iter 5000: 0.083502\n",
      "iter 5100: 0.083502\n",
      "iter 5200: 0.083502\n",
      "iter 5300: 0.083502\n",
      "iter 5400: 0.083502\n",
      "iter 5500: 0.083502\n",
      "iter 5600: 0.083502\n",
      "iter 5700: 0.083502\n",
      "iter 5800: 0.083502\n",
      "iter 5900: 0.083502\n",
      "iter 6000: 0.083502\n",
      "iter 6100: 0.083502\n",
      "iter 6200: 0.083502\n",
      "iter 6300: 0.083502\n",
      "iter 6400: 0.083502\n",
      "iter 6500: 0.083502\n",
      "iter 6600: 0.083502\n",
      "iter 6700: 0.083502\n",
      "iter 6800: 0.083502\n",
      "iter 6900: 0.083502\n",
      "iter 7000: 0.083502\n",
      "iter 7100: 0.083502\n",
      "iter 7200: 0.083502\n",
      "iter 7300: 0.083502\n",
      "iter 7400: 0.083502\n",
      "iter 7500: 0.083502\n",
      "iter 7600: 0.083502\n",
      "iter 7700: 0.083502\n",
      "iter 7800: 0.083502\n",
      "iter 7900: 0.083502\n",
      "iter 8000: 0.083502\n",
      "iter 8100: 0.083502\n",
      "iter 8200: 0.083502\n",
      "iter 8300: 0.083502\n",
      "iter 8400: 0.083502\n",
      "iter 8500: 0.083502\n",
      "iter 8600: 0.083502\n",
      "iter 8700: 0.083502\n",
      "iter 8800: 0.083502\n",
      "iter 8900: 0.083502\n",
      "iter 9000: 0.083502\n",
      "iter 9100: 0.083502\n",
      "iter 9200: 0.083502\n",
      "iter 9300: 0.083502\n",
      "iter 9400: 0.083502\n",
      "iter 9500: 0.083502\n",
      "iter 9600: 0.083502\n",
      "iter 9700: 0.083502\n",
      "iter 9800: 0.083502\n",
      "iter 9900: 0.083502\n",
      "iter 10000: 0.083502\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.037584\n",
      "iter 100: 0.133042\n",
      "iter 200: 0.133042\n",
      "iter 300: 0.133042\n",
      "iter 400: 0.133042\n",
      "iter 500: 0.133042\n",
      "iter 600: 0.133042\n",
      "iter 700: 0.133042\n",
      "iter 800: 0.133042\n",
      "iter 900: 0.133042\n",
      "iter 1000: 0.133042\n",
      "iter 1100: 0.133042\n",
      "iter 1200: 0.133042\n",
      "iter 1300: 0.133042\n",
      "iter 1400: 0.133042\n",
      "iter 1500: 0.133042\n",
      "iter 1600: 0.133042\n",
      "iter 1700: 0.133042\n",
      "iter 1800: 0.133042\n",
      "iter 1900: 0.133042\n",
      "iter 2000: 0.133042\n",
      "iter 2100: 0.133042\n",
      "iter 2200: 0.133042\n",
      "iter 2300: 0.133042\n",
      "iter 2400: 0.133042\n",
      "iter 2500: 0.133042\n",
      "iter 2600: 0.133042\n",
      "iter 2700: 0.133042\n",
      "iter 2800: 0.133042\n",
      "iter 2900: 0.133042\n",
      "iter 3000: 0.133042\n",
      "iter 3100: 0.133042\n",
      "iter 3200: 0.133042\n",
      "iter 3300: 0.133042\n",
      "iter 3400: 0.133042\n",
      "iter 3500: 0.133042\n",
      "iter 3600: 0.133042\n",
      "iter 3700: 0.133042\n",
      "iter 3800: 0.133042\n",
      "iter 3900: 0.133042\n",
      "iter 4000: 0.133042\n",
      "iter 4100: 0.133042\n",
      "iter 4200: 0.133042\n",
      "iter 4300: 0.133042\n",
      "iter 4400: 0.133042\n",
      "iter 4500: 0.133042\n",
      "iter 4600: 0.133042\n",
      "iter 4700: 0.133042\n",
      "iter 4800: 0.133042\n",
      "iter 4900: 0.133042\n",
      "iter 5000: 0.133042\n",
      "iter 5100: 0.133042\n",
      "iter 5200: 0.133042\n",
      "iter 5300: 0.133042\n",
      "iter 5400: 0.133042\n",
      "iter 5500: 0.133042\n",
      "iter 5600: 0.133042\n",
      "iter 5700: 0.133042\n",
      "iter 5800: 0.133042\n",
      "iter 5900: 0.133042\n",
      "iter 6000: 0.133042\n",
      "iter 6100: 0.133042\n",
      "iter 6200: 0.133042\n",
      "iter 6300: 0.133042\n",
      "iter 6400: 0.133042\n",
      "iter 6500: 0.133042\n",
      "iter 6600: 0.133042\n",
      "iter 6700: 0.133042\n",
      "iter 6800: 0.133042\n",
      "iter 6900: 0.133042\n",
      "iter 7000: 0.133042\n",
      "iter 7100: 0.133042\n",
      "iter 7200: 0.133042\n",
      "iter 7300: 0.133042\n",
      "iter 7400: 0.133042\n",
      "iter 7500: 0.133042\n",
      "iter 7600: 0.133042\n",
      "iter 7700: 0.133042\n",
      "iter 7800: 0.133042\n",
      "iter 7900: 0.133042\n",
      "iter 8000: 0.133042\n",
      "iter 8100: 0.133042\n",
      "iter 8200: 0.133042\n",
      "iter 8300: 0.133042\n",
      "iter 8400: 0.133042\n",
      "iter 8500: 0.133042\n",
      "iter 8600: 0.133042\n",
      "iter 8700: 0.133042\n",
      "iter 8800: 0.133042\n",
      "iter 8900: 0.133042\n",
      "iter 9000: 0.133042\n",
      "iter 9100: 0.133042\n",
      "iter 9200: 0.133042\n",
      "iter 9300: 0.133042\n",
      "iter 9400: 0.133042\n",
      "iter 9500: 0.133042\n",
      "iter 9600: 0.133042\n",
      "iter 9700: 0.133042\n",
      "iter 9800: 0.133042\n",
      "iter 9900: 0.133042\n",
      "iter 10000: 0.133042\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.075858\n",
      "iter 100: 0.208673\n",
      "iter 200: 0.208673\n",
      "iter 300: 0.208673\n",
      "iter 400: 0.208673\n",
      "iter 500: 0.208673\n",
      "iter 600: 0.208673\n",
      "iter 700: 0.208673\n",
      "iter 800: 0.208673\n",
      "iter 900: 0.208673\n",
      "iter 1000: 0.208673\n",
      "iter 1100: 0.208673\n",
      "iter 1200: 0.208673\n",
      "iter 1300: 0.208673\n",
      "iter 1400: 0.208673\n",
      "iter 1500: 0.208673\n",
      "iter 1600: 0.208673\n",
      "iter 1700: 0.208673\n",
      "iter 1800: 0.208673\n",
      "iter 1900: 0.208673\n",
      "iter 2000: 0.208673\n",
      "iter 2100: 0.208673\n",
      "iter 2200: 0.208673\n",
      "iter 2300: 0.208673\n",
      "iter 2400: 0.208673\n",
      "iter 2500: 0.208673\n",
      "iter 2600: 0.208673\n",
      "iter 2700: 0.208673\n",
      "iter 2800: 0.208673\n",
      "iter 2900: 0.208673\n",
      "iter 3000: 0.208673\n",
      "iter 3100: 0.208673\n",
      "iter 3200: 0.208673\n",
      "iter 3300: 0.208673\n",
      "iter 3400: 0.208673\n",
      "iter 3500: 0.208673\n",
      "iter 3600: 0.208673\n",
      "iter 3700: 0.208673\n",
      "iter 3800: 0.208673\n",
      "iter 3900: 0.208673\n",
      "iter 4000: 0.208673\n",
      "iter 4100: 0.208673\n",
      "iter 4200: 0.208673\n",
      "iter 4300: 0.208673\n",
      "iter 4400: 0.208673\n",
      "iter 4500: 0.208673\n",
      "iter 4600: 0.208673\n",
      "iter 4700: 0.208673\n",
      "iter 4800: 0.208673\n",
      "iter 4900: 0.208673\n",
      "iter 5000: 0.208673\n",
      "iter 5100: 0.208673\n",
      "iter 5200: 0.208673\n",
      "iter 5300: 0.208673\n",
      "iter 5400: 0.208673\n",
      "iter 5500: 0.208673\n",
      "iter 5600: 0.208673\n",
      "iter 5700: 0.208673\n",
      "iter 5800: 0.208673\n",
      "iter 5900: 0.208673\n",
      "iter 6000: 0.208673\n",
      "iter 6100: 0.208673\n",
      "iter 6200: 0.208673\n",
      "iter 6300: 0.208673\n",
      "iter 6400: 0.208673\n",
      "iter 6500: 0.208673\n",
      "iter 6600: 0.208673\n",
      "iter 6700: 0.208673\n",
      "iter 6800: 0.208673\n",
      "iter 6900: 0.208673\n",
      "iter 7000: 0.208673\n",
      "iter 7100: 0.208673\n",
      "iter 7200: 0.208673\n",
      "iter 7300: 0.208673\n",
      "iter 7400: 0.208673\n",
      "iter 7500: 0.208673\n",
      "iter 7600: 0.208673\n",
      "iter 7700: 0.208673\n",
      "iter 7800: 0.208673\n",
      "iter 7900: 0.208673\n",
      "iter 8000: 0.208673\n",
      "iter 8100: 0.208673\n",
      "iter 8200: 0.208673\n",
      "iter 8300: 0.208673\n",
      "iter 8400: 0.208673\n",
      "iter 8500: 0.208673\n",
      "iter 8600: 0.208673\n",
      "iter 8700: 0.208673\n",
      "iter 8800: 0.208673\n",
      "iter 8900: 0.208673\n",
      "iter 9000: 0.208673\n",
      "iter 9100: 0.208673\n",
      "iter 9200: 0.208673\n",
      "iter 9300: 0.208673\n",
      "iter 9400: 0.208673\n",
      "iter 9500: 0.208673\n",
      "iter 9600: 0.208673\n",
      "iter 9700: 0.208673\n",
      "iter 9800: 0.208673\n",
      "iter 9900: 0.208673\n",
      "iter 10000: 0.208673\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.153109\n",
      "iter 100: 0.320099\n",
      "iter 200: 0.320099\n",
      "iter 300: 0.320099\n",
      "iter 400: 0.320099\n",
      "iter 500: 0.320099\n",
      "iter 600: 0.320099\n",
      "iter 700: 0.320099\n",
      "iter 800: 0.320099\n",
      "iter 900: 0.320099\n",
      "iter 1000: 0.320099\n",
      "iter 1100: 0.320099\n",
      "iter 1200: 0.320099\n",
      "iter 1300: 0.320099\n",
      "iter 1400: 0.320099\n",
      "iter 1500: 0.320099\n",
      "iter 1600: 0.320099\n",
      "iter 1700: 0.320099\n",
      "iter 1800: 0.320099\n",
      "iter 1900: 0.320099\n",
      "iter 2000: 0.320099\n",
      "iter 2100: 0.320099\n",
      "iter 2200: 0.320099\n",
      "iter 2300: 0.320099\n",
      "iter 2400: 0.320099\n",
      "iter 2500: 0.320099\n",
      "iter 2600: 0.320099\n",
      "iter 2700: 0.320099\n",
      "iter 2800: 0.320099\n",
      "iter 2900: 0.320099\n",
      "iter 3000: 0.320099\n",
      "iter 3100: 0.320099\n",
      "iter 3200: 0.320099\n",
      "iter 3300: 0.320099\n",
      "iter 3400: 0.320099\n",
      "iter 3500: 0.320099\n",
      "iter 3600: 0.320099\n",
      "iter 3700: 0.320099\n",
      "iter 3800: 0.320099\n",
      "iter 3900: 0.320099\n",
      "iter 4000: 0.320099\n",
      "iter 4100: 0.320099\n",
      "iter 4200: 0.320099\n",
      "iter 4300: 0.320099\n",
      "iter 4400: 0.320099\n",
      "iter 4500: 0.320099\n",
      "iter 4600: 0.320099\n",
      "iter 4700: 0.320099\n",
      "iter 4800: 0.320099\n",
      "iter 4900: 0.320099\n",
      "iter 5000: 0.320099\n",
      "iter 5100: 0.320099\n",
      "iter 5200: 0.320099\n",
      "iter 5300: 0.320099\n",
      "iter 5400: 0.320099\n",
      "iter 5500: 0.320099\n",
      "iter 5600: 0.320099\n",
      "iter 5700: 0.320099\n",
      "iter 5800: 0.320099\n",
      "iter 5900: 0.320099\n",
      "iter 6000: 0.320099\n",
      "iter 6100: 0.320099\n",
      "iter 6200: 0.320099\n",
      "iter 6300: 0.320099\n",
      "iter 6400: 0.320099\n",
      "iter 6500: 0.320099\n",
      "iter 6600: 0.320099\n",
      "iter 6700: 0.320099\n",
      "iter 6800: 0.320099\n",
      "iter 6900: 0.320099\n",
      "iter 7000: 0.320099\n",
      "iter 7100: 0.320099\n",
      "iter 7200: 0.320099\n",
      "iter 7300: 0.320099\n",
      "iter 7400: 0.320099\n",
      "iter 7500: 0.320099\n",
      "iter 7600: 0.320099\n",
      "iter 7700: 0.320099\n",
      "iter 7800: 0.320099\n",
      "iter 7900: 0.320099\n",
      "iter 8000: 0.320099\n",
      "iter 8100: 0.320099\n",
      "iter 8200: 0.320099\n",
      "iter 8300: 0.320099\n",
      "iter 8400: 0.320099\n",
      "iter 8500: 0.320099\n",
      "iter 8600: 0.320099\n",
      "iter 8700: 0.320099\n",
      "iter 8800: 0.320099\n",
      "iter 8900: 0.320099\n",
      "iter 9000: 0.320099\n",
      "iter 9100: 0.320099\n",
      "iter 9200: 0.320099\n",
      "iter 9300: 0.320099\n",
      "iter 9400: 0.320099\n",
      "iter 9500: 0.320099\n",
      "iter 9600: 0.320099\n",
      "iter 9700: 0.320099\n",
      "iter 9800: 0.320099\n",
      "iter 9900: 0.320099\n",
      "iter 10000: 0.320099\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.309030\n",
      "iter 100: 3.028372\n",
      "iter 200: 3.028372\n",
      "iter 300: 3.028372\n",
      "iter 400: 3.028372\n",
      "iter 500: 3.028372\n",
      "iter 600: 3.028372\n",
      "iter 700: 3.028372\n",
      "iter 800: 3.028372\n",
      "iter 900: 3.028372\n",
      "iter 1000: 3.028372\n",
      "iter 1100: 3.028372\n",
      "iter 1200: 3.028372\n",
      "iter 1300: 3.028372\n",
      "iter 1400: 3.028372\n",
      "iter 1500: 3.028372\n",
      "iter 1600: 3.028372\n",
      "iter 1700: 3.028372\n",
      "iter 1800: 3.028372\n",
      "iter 1900: 3.028372\n",
      "iter 2000: 3.028372\n",
      "iter 2100: 3.028372\n",
      "iter 2200: 3.028372\n",
      "iter 2300: 3.028372\n",
      "iter 2400: 3.028372\n",
      "iter 2500: 3.028372\n",
      "iter 2600: 3.028372\n",
      "iter 2700: 3.028372\n",
      "iter 2800: 3.028372\n",
      "iter 2900: 3.028372\n",
      "iter 3000: 3.028372\n",
      "iter 3100: 3.028372\n",
      "iter 3200: 3.028372\n",
      "iter 3300: 3.028372\n",
      "iter 3400: 3.028372\n",
      "iter 3500: 3.028372\n",
      "iter 3600: 3.028372\n",
      "iter 3700: 3.028372\n",
      "iter 3800: 3.028372\n",
      "iter 3900: 3.028372\n",
      "iter 4000: 3.028372\n",
      "iter 4100: 3.028372\n",
      "iter 4200: 3.028372\n",
      "iter 4300: 3.028372\n",
      "iter 4400: 3.028372\n",
      "iter 4500: 3.028372\n",
      "iter 4600: 3.028372\n",
      "iter 4700: 3.028372\n",
      "iter 4800: 3.028372\n",
      "iter 4900: 3.028372\n",
      "iter 5000: 3.028372\n",
      "iter 5100: 3.028372\n",
      "iter 5200: 3.028372\n",
      "iter 5300: 3.028372\n",
      "iter 5400: 3.028372\n",
      "iter 5500: 3.028372\n",
      "iter 5600: 3.028372\n",
      "iter 5700: 3.028372\n",
      "iter 5800: 3.028372\n",
      "iter 5900: 3.028372\n",
      "iter 6000: 3.028372\n",
      "iter 6100: 3.028372\n",
      "iter 6200: 3.028372\n",
      "iter 6300: 3.028372\n",
      "iter 6400: 3.028372\n",
      "iter 6500: 3.028372\n",
      "iter 6600: 3.028372\n",
      "iter 6700: 3.028372\n",
      "iter 6800: 3.028372\n",
      "iter 6900: 3.028372\n",
      "iter 7000: 3.028372\n",
      "iter 7100: 3.028372\n",
      "iter 7200: 3.028372\n",
      "iter 7300: 3.028372\n",
      "iter 7400: 3.028372\n",
      "iter 7500: 3.028372\n",
      "iter 7600: 3.028372\n",
      "iter 7700: 3.028372\n",
      "iter 7800: 3.028372\n",
      "iter 7900: 3.028372\n",
      "iter 8000: 3.028372\n",
      "iter 8100: 3.028372\n",
      "iter 8200: 3.028372\n",
      "iter 8300: 3.028372\n",
      "iter 8400: 3.028372\n",
      "iter 8500: 3.028372\n",
      "iter 8600: 3.028372\n",
      "iter 8700: 3.028372\n",
      "iter 8800: 3.028372\n",
      "iter 8900: 3.028372\n",
      "iter 9000: 3.028372\n",
      "iter 9100: 3.028372\n",
      "iter 9200: 3.028372\n",
      "iter 9300: 3.028372\n",
      "iter 9400: 3.028372\n",
      "iter 9500: 3.028372\n",
      "iter 9600: 3.028372\n",
      "iter 9700: 3.028372\n",
      "iter 9800: 3.028372\n",
      "iter 9900: 3.028372\n",
      "iter 10000: 3.028372\n",
      "Train accuracy (%): 99.976592\n",
      "Dev accuracy (%): 100.000000\n",
      "Training for reg=0.623735\n",
      "iter 100: 394.943741\n",
      "iter 200: 394.943784\n",
      "iter 300: 394.943826\n",
      "iter 400: 394.943865\n",
      "iter 500: 394.943903\n",
      "iter 600: 394.943938\n",
      "iter 700: 394.943972\n",
      "iter 800: 394.944004\n",
      "iter 900: 394.944035\n",
      "iter 1000: 394.944064\n",
      "iter 1100: 394.944091\n",
      "iter 1200: 394.944117\n",
      "iter 1300: 394.944142\n",
      "iter 1400: 394.944166\n",
      "iter 1500: 394.944188\n",
      "iter 1600: 394.944209\n",
      "iter 1700: 394.944230\n",
      "iter 1800: 394.944249\n",
      "iter 1900: 394.944267\n",
      "iter 2000: 394.944285\n",
      "iter 2100: 394.944301\n",
      "iter 2200: 394.944317\n",
      "iter 2300: 394.944332\n",
      "iter 2400: 394.944346\n",
      "iter 2500: 394.944359\n",
      "iter 2600: 394.944372\n",
      "iter 2700: 394.944384\n",
      "iter 2800: 394.944395\n",
      "iter 2900: 394.944406\n",
      "iter 3000: 394.944417\n",
      "iter 3100: 394.944427\n",
      "iter 3200: 394.944436\n",
      "iter 3300: 394.944445\n",
      "iter 3400: 394.944453\n",
      "iter 3500: 394.944461\n",
      "iter 3600: 394.944469\n",
      "iter 3700: 394.944476\n",
      "iter 3800: 394.944483\n",
      "iter 3900: 394.944490\n",
      "iter 4000: 394.944496\n",
      "iter 4100: 394.944502\n",
      "iter 4200: 394.944508\n",
      "iter 4300: 394.944513\n",
      "iter 4400: 394.944518\n",
      "iter 4500: 394.944523\n",
      "iter 4600: 394.944527\n",
      "iter 4700: 394.944532\n",
      "iter 4800: 394.944536\n",
      "iter 4900: 394.944540\n",
      "iter 5000: 394.944543\n",
      "iter 5100: 394.944547\n",
      "iter 5200: 394.944550\n",
      "iter 5300: 394.944554\n",
      "iter 5400: 394.944557\n",
      "iter 5500: 394.944559\n",
      "iter 5600: 394.944562\n",
      "iter 5700: 394.944565\n",
      "iter 5800: 394.944567\n",
      "iter 5900: 394.944570\n",
      "iter 6000: 394.944572\n",
      "iter 6100: 394.944574\n",
      "iter 6200: 394.944576\n",
      "iter 6300: 394.944578\n",
      "iter 6400: 394.944580\n",
      "iter 6500: 394.944581\n",
      "iter 6600: 394.944583\n",
      "iter 6700: 394.944585\n",
      "iter 6800: 394.944586\n",
      "iter 6900: 394.944587\n",
      "iter 7000: 394.944589\n",
      "iter 7100: 394.944590\n",
      "iter 7200: 394.944591\n",
      "iter 7300: 394.944592\n",
      "iter 7400: 394.944594\n",
      "iter 7500: 394.944595\n",
      "iter 7600: 394.944596\n",
      "iter 7700: 394.944596\n",
      "iter 7800: 394.944597\n",
      "iter 7900: 394.944598\n",
      "iter 8000: 394.944599\n",
      "iter 8100: 394.944600\n",
      "iter 8200: 394.944600\n",
      "iter 8300: 394.944601\n",
      "iter 8400: 394.944602\n",
      "iter 8500: 394.944602\n",
      "iter 8600: 394.944603\n",
      "iter 8700: 394.944604\n",
      "iter 8800: 394.944604\n",
      "iter 8900: 394.944605\n",
      "iter 9000: 394.944605\n",
      "iter 9100: 394.944606\n",
      "iter 9200: 394.944606\n",
      "iter 9300: 394.944606\n",
      "iter 9400: 394.944607\n",
      "iter 9500: 394.944607\n",
      "iter 9600: 394.944608\n",
      "iter 9700: 394.944608\n",
      "iter 9800: 394.944608\n",
      "iter 9900: 394.944608\n",
      "iter 10000: 394.944609\n",
      "Train accuracy (%): 0.023408\n",
      "Dev accuracy (%): 0.000000\n",
      "Training for reg=1.258925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandeep\\Anaconda3\\New folder\\assignment1\\q1_softmax.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  x = np.exp(x)\n",
      "C:\\Users\\Sandeep\\Anaconda3\\New folder\\assignment1\\q1_softmax.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x /= np.sum(x, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: nan\n",
      "iter 200: nan\n",
      "iter 300: nan\n",
      "iter 400: nan\n",
      "iter 500: nan\n",
      "iter 600: nan\n",
      "iter 700: nan\n",
      "iter 800: nan\n",
      "iter 900: nan\n",
      "iter 1000: nan\n",
      "iter 1100: nan\n",
      "iter 1200: nan\n",
      "iter 1300: nan\n",
      "iter 1400: nan\n",
      "iter 1500: nan\n",
      "iter 1600: nan\n",
      "iter 1700: nan\n",
      "iter 1800: nan\n",
      "iter 1900: nan\n",
      "iter 2000: nan\n",
      "iter 2100: nan\n",
      "iter 2200: nan\n",
      "iter 2300: nan\n",
      "iter 2400: nan\n",
      "iter 2500: nan\n",
      "iter 2600: nan\n",
      "iter 2700: nan\n",
      "iter 2800: nan\n",
      "iter 2900: nan\n",
      "iter 3000: nan\n",
      "iter 3100: nan\n",
      "iter 3200: nan\n",
      "iter 3300: nan\n",
      "iter 3400: nan\n",
      "iter 3500: nan\n",
      "iter 3600: nan\n",
      "iter 3700: nan\n",
      "iter 3800: nan\n",
      "iter 3900: nan\n",
      "iter 4000: nan\n",
      "iter 4100: nan\n",
      "iter 4200: nan\n",
      "iter 4300: nan\n",
      "iter 4400: nan\n",
      "iter 4500: nan\n",
      "iter 4600: nan\n",
      "iter 4700: nan\n",
      "iter 4800: nan\n",
      "iter 4900: nan\n",
      "iter 5000: nan\n",
      "iter 5100: nan\n",
      "iter 5200: nan\n",
      "iter 5300: nan\n",
      "iter 5400: nan\n",
      "iter 5500: nan\n",
      "iter 5600: nan\n",
      "iter 5700: nan\n",
      "iter 5800: nan\n",
      "iter 5900: nan\n",
      "iter 6000: nan\n",
      "iter 6100: nan\n",
      "iter 6200: nan\n",
      "iter 6300: nan\n",
      "iter 6400: nan\n",
      "iter 6500: nan\n",
      "iter 6600: nan\n",
      "iter 6700: nan\n",
      "iter 6800: nan\n",
      "iter 6900: nan\n",
      "iter 7000: nan\n",
      "iter 7100: nan\n",
      "iter 7200: nan\n",
      "iter 7300: nan\n",
      "iter 7400: nan\n",
      "iter 7500: nan\n",
      "iter 7600: nan\n",
      "iter 7700: nan\n",
      "iter 7800: nan\n",
      "iter 7900: nan\n",
      "iter 8000: nan\n",
      "iter 8100: nan\n",
      "iter 8200: nan\n",
      "iter 8300: nan\n",
      "iter 8400: nan\n",
      "iter 8500: nan\n",
      "iter 8600: nan\n",
      "iter 8700: nan\n",
      "iter 8800: nan\n",
      "iter 8900: nan\n",
      "iter 9000: nan\n",
      "iter 9100: nan\n",
      "iter 9200: nan\n",
      "iter 9300: nan\n",
      "iter 9400: nan\n",
      "iter 9500: nan\n",
      "iter 9600: nan\n",
      "iter 9700: nan\n",
      "iter 9800: nan\n",
      "iter 9900: nan\n",
      "iter 10000: nan\n",
      "Train accuracy (%): 100.000000\n",
      "Dev accuracy (%): 100.000000\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\t\tDev\n",
      "0.000000E+00\t99.98\t100\n",
      "1.000000E-06\t99.98\t100\n",
      "2.018366E-06\t99.98\t100\n",
      "4.073803E-06\t99.98\t100\n",
      "8.222426E-06\t99.98\t100\n",
      "1.659587E-05\t99.98\t100\n",
      "3.349654E-05\t99.98\t100\n",
      "6.760830E-05\t99.98\t100\n",
      "1.364583E-04\t99.98\t100\n",
      "2.754229E-04\t99.98\t100\n",
      "5.559043E-04\t99.98\t100\n",
      "1.122018E-03\t99.98\t100\n",
      "2.264644E-03\t99.98\t100\n",
      "4.570882E-03\t99.98\t100\n",
      "9.225714E-03\t99.98\t100\n",
      "1.862087E-02\t99.98\t100\n",
      "3.758374E-02\t99.98\t100\n",
      "7.585776E-02\t99.98\t100\n",
      "1.531087E-01\t99.98\t100\n",
      "3.090295E-01\t99.98\t100\n",
      "6.237348E-01\t0.02341\t0\n",
      "1.258925E+00\t100\t100\n",
      "0.00e+00 & 99.98 & 100 \\\\\n",
      "1.00e-06 & 99.98 & 100 \\\\\n",
      "2.02e-06 & 99.98 & 100 \\\\\n",
      "4.07e-06 & 99.98 & 100 \\\\\n",
      "8.22e-06 & 99.98 & 100 \\\\\n",
      "1.66e-05 & 99.98 & 100 \\\\\n",
      "3.35e-05 & 99.98 & 100 \\\\\n",
      "6.76e-05 & 99.98 & 100 \\\\\n",
      "1.36e-04 & 99.98 & 100 \\\\\n",
      "2.75e-04 & 99.98 & 100 \\\\\n",
      "5.56e-04 & 99.98 & 100 \\\\\n",
      "1.12e-03 & 99.98 & 100 \\\\\n",
      "2.26e-03 & 99.98 & 100 \\\\\n",
      "4.57e-03 & 99.98 & 100 \\\\\n",
      "9.23e-03 & 99.98 & 100 \\\\\n",
      "1.86e-02 & 99.98 & 100 \\\\\n",
      "3.76e-02 & 99.98 & 100 \\\\\n",
      "7.59e-02 & 99.98 & 100 \\\\\n",
      "1.53e-01 & 99.98 & 100 \\\\\n",
      "3.09e-01 & 99.98 & 100 \\\\\n",
      "6.24e-01 & 0.02341 & 0 \\\\\n",
      "1.26e+00 & 100 & 100 \\\\\n",
      "\n",
      "Best regularization value: 0.000000E+00\n",
      "Test accuracy (%): 100.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAF/CAYAAAB9pwLkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n80MUcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x220d6d94908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs224d.data_utils import *\n",
    "\n",
    "from q3_sgd import load_saved_params, sgd\n",
    "from q4_softmaxreg import softmaxRegression, getSentenceFeature, accuracy, softmax_wrapper\n",
    "\n",
    "\n",
    "\n",
    "# Try different regularizations and pick the best!\n",
    "# NOTE: fill in one more \"your code here\" below before running!\n",
    "REGULARIZATION = None   # Assign a list of floats in the block below\n",
    "### YOUR CODE HERE\n",
    "REGULARIZATION = np.logspace(-6,0.1,21)\n",
    "REGULARIZATION = np.hstack([0,REGULARIZATION])\n",
    "### END YOUR CODE\n",
    "\n",
    "# Load the dataset\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# Load the word vectors we trained earlier\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "N = wordVectors0.shape[0]//2\n",
    "#assert nWords == N\n",
    "wordVectors = (wordVectors0[:N,:] + wordVectors0[N:,:])\n",
    "dimVectors = wordVectors.shape[1]\n",
    "\n",
    "# Load the train set\n",
    "trainset = dataset.getTrainSentences()\n",
    "nTrain = len(trainset)\n",
    "trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "for i in range(nTrain):\n",
    "    words, trainLabels[i] = trainset[i]\n",
    "    trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Prepare dev set features\n",
    "devset = dataset.getDevSentences()\n",
    "nDev = len(devset)\n",
    "devFeatures = np.zeros((nDev, dimVectors))\n",
    "devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "for i in range(nDev):\n",
    "    words, devLabels[i] = devset[i]\n",
    "    devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Try our regularization parameters\n",
    "results = []\n",
    "for regularization in REGULARIZATION:\n",
    "    random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "    print(\"Training for reg=%f\" % regularization)\n",
    "\n",
    "    # We will do batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels,\n",
    "        weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "    # Test on train set\n",
    "    _, _, pred = softmaxRegression(trainFeatures, trainLabels, weights)\n",
    "    trainAccuracy = accuracy(trainLabels, pred)\n",
    "    print(\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "    # Test on dev set\n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    devAccuracy = accuracy(devLabels, pred)\n",
    "    print(\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "    # Save the results and weights\n",
    "    results.append({\n",
    "        \"reg\" : regularization,\n",
    "        \"weights\" : weights,\n",
    "        \"train\" : trainAccuracy,\n",
    "        \"dev\" : devAccuracy})\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"\")\n",
    "print(\"=== Recap ===\")\n",
    "print(\"Reg\\t\\tTrain\\t\\tDev\")\n",
    "for result in results:\n",
    "    print(\"%E\\t%0.4g\\t%0.4g\" % (\n",
    "        result[\"reg\"],\n",
    "        result[\"train\"],\n",
    "        result[\"dev\"]))\n",
    "for result in results:\n",
    "    print(\"%0.2e & %0.4g & %0.4g \\\\\\\\\" % (\n",
    "        result[\"reg\"],\n",
    "        result[\"train\"],\n",
    "        result[\"dev\"]))\n",
    "print(\"\")\n",
    "\n",
    "# Pick the best regularization parameters\n",
    "BEST_REGULARIZATION = None\n",
    "BEST_WEIGHTS = None\n",
    "\n",
    "### YOUR CODE HERE\n",
    "sorted_results = sorted(results, key=lambda x: x['dev'],reverse=True)\n",
    "BEST_REGULARIZATION = sorted_results[0]['reg']\n",
    "BEST_WEIGHTS = sorted_results[0]['weights']\n",
    "### END YOUR CODE\n",
    "\n",
    "# Test your findings on the test set\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "for i in range(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, BEST_WEIGHTS)\n",
    "print(\"Best regularization value: %E\" % BEST_REGULARIZATION)\n",
    "print(\"Test accuracy (%%): %f\" % accuracy(testLabels, pred))\n",
    "\n",
    "# Make a plot of regularization vs accuracy\n",
    "plt.plot(REGULARIZATION, [x[\"train\"] for x in results])\n",
    "plt.plot(REGULARIZATION, [x[\"dev\"] for x in results])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"regularization\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(['train', 'dev'], loc='upper right')\n",
    "plt.savefig(\"q4_reg_v_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
